{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nimfa \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-2]) + '/data/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(3, '/'.join(os.getcwd().split(\"/\")[:-2]) + '/textacy')\n",
    "\n",
    "import textacy\n",
    "from textacy import preprocess_text, Doc, Corpus\n",
    "from textacy.vsm import Vectorizer, GroupVectorizer\n",
    "from textacy.tm import TopicModel\n",
    "en = textacy.load_spacy(\"en_core_web_sm\", disable='parser')\n",
    "\n",
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169, 8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_directory+\"tokenized_docs.p\", \"rb\") as f:\n",
    "        docs = pkl.load(f)\n",
    "        \n",
    "with open(data_directory+\"cleaned_data.p\", \"rb\") as f:\n",
    "        orig_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(Phrases([doc[0] for doc in docs], min_count=10, threshold=20, delimiter=b' '))\n",
    "bigram_docs = [bigram_phraser[doc[0]] for doc in docs] \n",
    "\n",
    "trigram_phraser = Phraser(Phrases(bigram_docs, min_count=5, threshold=10, delimiter=b' '))\n",
    "trigram_docs = [trigram_phraser[doc] for doc in bigram_docs] \n",
    "\n",
    "analysts = [d[1]['AnalystName'] for d in docs]\n",
    "tags = [d[1]['Tag'] for d in docs]\n",
    "companies = [d[1]['Company'] for d in docs]\n",
    "quarters = [d[1]['Quarter'] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_RANK = 4\n",
    "\n",
    "a_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, analysts)\n",
    "a_doc_term_matrix = a_vec.transform(trigram_docs, analysts)\n",
    "\n",
    "a_mod = nimfa.Lsnmf(V=a_doc_term_matrix, max_iter=1000, rank=A_RANK, n_run=10)\n",
    "a_mod_fit = a_mod()\n",
    "\n",
    "a_df = pd.SparseDataFrame(normalize(a_mod_fit.basis()), columns = ['aTopic'+str(i) for i in range(A_RANK)], index=a_vec.grps_list).fillna(0)\n",
    "a_df.index.name = 'AnalystName'\n",
    "#a_df.join(a_df.idxmax(axis=1).rename('aTopicMax')).reset_index().to_csv(data_directory+\"analystTopic.csv\", index=False)\n",
    "a_df.reset_index().to_csv(data_directory+\"analystTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANK = 6\n",
    "\n",
    "t_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, tags)\n",
    "t_doc_term_matrix = t_vec.transform(trigram_docs, tags)\n",
    "\n",
    "t_mod = nimfa.Lsnmf(V=t_doc_term_matrix, max_iter=1000, rank=T_RANK, n_run=10)\n",
    "t_mod_fit = t_mod()\n",
    "\n",
    "t_df = pd.SparseDataFrame(normalize(t_mod_fit.basis()), columns = ['tTopic'+str(i) for i in range(T_RANK)], index=t_vec.grps_list).fillna(0)\n",
    "t_df.index.name = 'Tag'\n",
    "#t_df.join(t_df.idxmax(axis=1).rename('tTopicMax')).reset_index().to_csv(data_directory+\"tagTopic.csv\", index=False)\n",
    "t_df.reset_index().to_csv(data_directory+\"tagTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_RANK = 4\n",
    "\n",
    "c_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, companies)\n",
    "c_doc_term_matrix = c_vec.transform(trigram_docs, companies)\n",
    "\n",
    "c_mod = nimfa.Lsnmf(V=c_doc_term_matrix, max_iter=1000, rank=C_RANK, n_run=10)\n",
    "c_mod_fit = c_mod()\n",
    "\n",
    "c_df = pd.SparseDataFrame(normalize(c_mod_fit.basis()), columns = ['cTopic'+str(i) for i in range(C_RANK)], index=c_vec.grps_list).fillna(0)\n",
    "c_df.index.name = 'Company'\n",
    "#c_df.join(c_df.idxmax(axis=1).rename('cTopicMax')).reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)\n",
    "c_df.reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_RANK = 4\n",
    "\n",
    "q_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, quarters)\n",
    "q_doc_term_matrix = q_vec.transform(trigram_docs, quarters)\n",
    "\n",
    "q_mod = nimfa.Lsnmf(V=q_doc_term_matrix, max_iter=1000, rank=Q_RANK, n_run=10)\n",
    "q_mod_fit = q_mod()\n",
    "\n",
    "q_df = pd.SparseDataFrame(normalize(q_mod_fit.basis()), columns = ['qTopic'+str(i) for i in range(Q_RANK)], index=q_vec.grps_list).fillna(0)\n",
    "q_df.index.name = 'Quarter'\n",
    "#c_df.join(c_df.idxmax(axis=1).rename('cTopicMax')).reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)\n",
    "q_df.reset_index().to_csv(data_directory+\"quarterTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_save = ['EventNumber', 'Company', 'AnalystName', 'Month', 'Year', 'Quarter', 'EventType', 'Date'] \n",
    "\n",
    "pivot_data = pd.pivot_table(orig_data, index=cols_save, columns=['Tag'], aggfunc='size', fill_value=0)\n",
    "pivot_data.reset_index(inplace=True)\n",
    "pivot_data.columns = pivot_data.columns.get_level_values(0)\n",
    "pivot_sum_data = pivot_data.groupby(cols_save).sum(axis=1).reset_index()\n",
    "\n",
    "melt_data = pd.melt(pivot_sum_data, id_vars=cols_save, var_name=['Tag'], value_name='NumQ')\n",
    "melt_data['NumQ'] = melt_data['NumQ'].astype(bool).astype(int)\n",
    "melt_data['Tag'] = melt_data['Tag'].str.split(\"_\").str[-1]\n",
    "\n",
    "analyst_data = pd.read_csv(data_directory+\"analystTopic.csv\")\n",
    "tag_data = pd.read_csv(data_directory+\"tagTopic.csv\")\n",
    "company_data = pd.read_csv(data_directory+\"companyTopic.csv\")\n",
    "quarter_data = pd.read_csv(data_directory+\"quarterTopic.csv\")\n",
    "\n",
    "a_topic_cols = analyst_data.drop(['AnalystName'], axis=1).columns.tolist()\n",
    "t_topic_cols = tag_data.drop(['Tag'], axis=1).columns.tolist()\n",
    "c_topic_cols = company_data.drop(['Company'], axis=1).columns.tolist()\n",
    "q_topic_cols = quarter_data.drop(['Quarter'], axis=1).columns.tolist()\n",
    "\n",
    "merge_data = melt_data.merge(analyst_data, on=['AnalystName'])\n",
    "merge_data = merge_data.merge(tag_data, on=['Tag'])\n",
    "merge_data = merge_data.merge(company_data, on=['Company'])\n",
    "merge_data = merge_data.merge(quarter_data, on=['Quarter'])\n",
    "\n",
    "merge_data = pd.concat([merge_data,\n",
    "                      pd.get_dummies(merge_data[[\"EventType\"]])], axis=1).reset_index(drop=True)\n",
    "\n",
    "#features_data = merge_data.drop(['Company', 'AnalystName', 'QuestionNumber' ,'EventType', 'Date', 'Tag', 'Month', 'Year'], axis=1).drop_duplicates().copy()\n",
    "features_data = merge_data.drop(['Company', 'AnalystName' ,'EventType', 'Date', 'Tag', 'Month', 'Year'], axis=1).copy()\n",
    "features_data[a_topic_cols + t_topic_cols + c_topic_cols + q_topic_cols] = features_data[a_topic_cols + t_topic_cols + c_topic_cols + q_topic_cols]**2\n",
    "train, test = features_data.loc[~features_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True), \\\n",
    "                features_data.loc[features_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ','EventNumber'], axis=1), train['NumQ']\n",
    "X_test, y_test = test.drop(['NumQ', 'EventNumber'], axis=1), test['NumQ']\n",
    "\n",
    "cols_list = train.drop(['NumQ','EventNumber'], axis=1).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7751641003840418\n",
      "Accuracy: 0.8576371725160653\n"
     ]
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, n_estimators=1000, n_iter_no_change=5).fit(X_train, y_train)\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7398531562754437\n",
      "Accuracy: 0.8386060306475531\n",
      "0. aTopic1: 0.1108 +/- 0.01477\n",
      "1. aTopic0: 0.1103 +/- 0.01476\n",
      "2. aTopic2: 0.1094 +/- 0.01486\n",
      "3. aTopic3: 0.1078 +/- 0.01426\n",
      "4. cTopic0: 0.04995 +/- 0.0114\n",
      "5. cTopic3: 0.04688 +/- 0.01006\n",
      "6. cTopic1: 0.0431 +/- 0.009839\n",
      "7. tTopic4: 0.04289 +/- 0.01691\n",
      "8. tTopic0: 0.03897 +/- 0.01648\n",
      "9. tTopic1: 0.0387 +/- 0.01416\n",
      "10. EventType_Conference: 0.03677 +/- 0.01477\n",
      "11. tTopic5: 0.03363 +/- 0.01317\n",
      "12. tTopic2: 0.03314 +/- 0.01547\n",
      "13. cTopic2: 0.03279 +/- 0.007383\n",
      "14. tTopic3: 0.0295 +/- 0.01254\n",
      "15. qTopic3: 0.02187 +/- 0.006052\n",
      "16. qTopic0: 0.02171 +/- 0.005962\n",
      "17. Quarter: 0.02159 +/- 0.005959\n",
      "18. qTopic2: 0.02158 +/- 0.006047\n",
      "19. EventType_EarningsCall: 0.0195 +/- 0.009614\n",
      "20. qTopic1: 0.01774 +/- 0.004891\n",
      "21. EventType_FixedIncomeCall: 0.00589 +/- 0.00311\n",
      "22. EventType_InvestorDay: 0.003888 +/- 0.002107\n",
      "23. EventType_Other: 0.001598 +/- 0.001115\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestClassifier(warm_start=True, n_estimators=1000).fit(X_train, y_train)\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))\n",
    "\n",
    "importances = estimator.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in estimator.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(indices.shape[0]):\n",
    "    \n",
    "    print(\"{}. {}: {:.4} +/- {:.4}\".format(f, cols_list[indices[f]], importances[indices[f]], std[indices[f]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=1.0, loss='deviance', max_depth=25,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=25, min_samples_split=25,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "              n_iter_no_change=5, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, \n",
    "                                       n_estimators=1000,\n",
    "                                       learning_rate=1.0,\n",
    "                                       max_depth=25,\n",
    "                                       min_samples_leaf=25,\n",
    "                                       n_iter_no_change=5)\n",
    "\n",
    "param_grid = {'min_samples_split':[2,3,5,10,15,25]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator, \n",
    "                    param_grid,\n",
    "                    make_scorer(roc_auc_score, greater_is_better=True),\n",
    "                    cv = 5,\n",
    "                    return_train_score=False)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7847418466186795\n",
      "Accuracy: 0.8608502224419179\n"
     ]
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, \n",
    "                                       n_estimators=1000,\n",
    "                                       learning_rate=0.01,\n",
    "                                       max_depth=25,\n",
    "                                       max_features='auto',\n",
    "                                       min_samples_leaf=25,\n",
    "                                       min_samples_split=10, \n",
    "                                       n_iter_no_change=5).fit(X_train, y_train)\n",
    "\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7884823506524385\n",
      "Accuracy: 0.856401384083045\n"
     ]
    }
   ],
   "source": [
    "calibrated = CalibratedClassifierCV(estimator, method='sigmoid', cv=5).fit(X_train, y_train)\n",
    "\n",
    "preds = calibrated.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_data.drop(['NumQ', 'EventNumber'], axis=1).copy()\n",
    "y = features_data['NumQ'].copy()\n",
    "\n",
    "estimator = GradientBoostingClassifier(warm_start=True, n_estimators=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ind_data = pd.read_csv(data_directory + 'qaData.csv', parse_dates=['Date'])\n",
    "\n",
    "q_ind_data['EarningTag2'] = q_ind_data['EarningTag2'].str.strip()\n",
    "q_ind_data['Year'] = q_ind_data['Date'].dt.year\n",
    "q_ind_data['Month'] = q_ind_data['Date'].dt.month\n",
    "q_ind_data['Quarter'] = q_ind_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "q_ind_data['Company'] = q_ind_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['EventType'] = q_ind_data['EventType'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['AnalystName'] = q_ind_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['Tag'] = q_ind_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "q_ind_data = q_ind_data.loc[~q_ind_data['AnalystName'].isna()].copy()\n",
    "\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(q_ind_data.groupby(['Company', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    g2.reset_index(drop=True, inplace=True)\n",
    "    g2.index.name = \"QuestionNumber\"\n",
    "    g2.reset_index(inplace=True)\n",
    "    groups.append(g2)\n",
    "\n",
    "q_ind_data = pd.concat(groups)[['EventNumber', 'QuestionNumber', 'Company', 'Month', 'Year', 'Quarter', 'EventType', 'Date', 'AnalystName', \"Tag\", \"Question\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec = Vectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs)\n",
    "q_doc_term_matrix = q_vec.transform(trigram_docs)\n",
    "\n",
    "t_affinity_mat = 1- sp.spatial.distance.cdist(q_doc_term_matrix.toarray(), t_doc_term_matrix.toarray(), 'cosine')\n",
    "t_affinity = pd.SparseDataFrame(t_affinity_mat, columns=t_vec.grps_list)\n",
    "\n",
    "a_affinity_mat = 1- sp.spatial.distance.cdist(q_doc_term_matrix.toarray(), a_doc_term_matrix.toarray(), 'cosine')\n",
    "a_affinity = pd.SparseDataFrame(a_affinity_mat, columns=a_vec.grps_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tTopicMax_tTopic0' 'tTopicMax_tTopic1' 'tTopicMax_tTopic2'\\n 'tTopicMax_tTopic3' 'tTopicMax_tTopic4' 'tTopicMax_tTopic5'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-4f3e11c2a61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyst_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tTopicMax'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tTopicMax_tTopic'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tTopicMax'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tTopicMax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tTopicMax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tTopicMax_tTopic0' 'tTopicMax_tTopic1' 'tTopicMax_tTopic2'\\n 'tTopicMax_tTopic3' 'tTopicMax_tTopic4' 'tTopicMax_tTopic5'] not in index\""
     ]
    }
   ],
   "source": [
    "event = features_data.loc[features_data['EventNumber']==155].reset_index(drop=True).copy()\n",
    "X_event = event.drop(['NumQ', 'EventNumber'], axis=1).copy()\n",
    "y_event = event['NumQ'].copy()\n",
    "predictions = pd.Series(estimator.predict_proba(X_event)[:,1]).rename('Prediction')\n",
    "\n",
    "event = pd.concat([event, predictions], axis=1)\n",
    "event = pd.merge(event, analyst_data)\n",
    "event['tTopicMax'] = event[['tTopicMax_tTopic'+str(i) for i in range(6)]].idxmax(1)\n",
    "event['tTopicMax'] = event['tTopicMax'].str.split(\"_\").str[-1]\n",
    "event = pd.merge(event, tag_data[['tTopicMax', 'Tag']]).drop_duplicates()\n",
    "\n",
    "top3 = event.groupby(['AnalystName']).apply(lambda x: x[['Tag', 'Prediction']].nlargest(2, 'Prediction')).reset_index()[['AnalystName', 'Tag', 'Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_q = {}\n",
    "\n",
    "for ind in top3[['AnalystName', 'Tag']].values:\n",
    "    a, t = ind\n",
    "    if a not in pred_q:\n",
    "        pred_q[a] = {}\n",
    "    \n",
    "    pred_q[a][t] = []\n",
    "    affinities = (a_affinity[a] + t_affinity[t])/2\n",
    "    q_ind = affinities.nlargest(1).reset_index()['index'].values\n",
    "\n",
    "    for val in q_ind:\n",
    "        #' '.join(docs[val][0])\n",
    "        question = q_ind_data.loc[(q_ind_data['EventNumber']==docs[val][1]['EventNumber']) & \n",
    "                       (q_ind_data['QuestionNumber']==docs[val][1]['QuestionNumber']),'Question'].item()\n",
    "        pred_q[a][t].append((question, affinities[val]))\n",
    "\n",
    "for a, t_dict in pred_q.items():\n",
    "    print(\"Analyst: {}\".format(a))\n",
    "    for t, values in pred_q[a].items():\n",
    "        prob = top3.loc[(top3['AnalystName']==a) & (top3['Tag']==t),'Prediction'].item()\n",
    "        print(\"\\tTag: {} ({:.4})\".format(t, prob))\n",
    "        for v0, v1 in values:\n",
    "            print(\"\\t\\t({:.4}) - {}\".format(v1, v0))\n",
    "    print(\"*********************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
