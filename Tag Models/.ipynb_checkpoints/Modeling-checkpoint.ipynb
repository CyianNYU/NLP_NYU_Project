{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nimfa \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-2]) + '/data/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(3, '/'.join(os.getcwd().split(\"/\")[:-2]) + '/textacy')\n",
    "\n",
    "import textacy\n",
    "from textacy import preprocess_text, Doc, Corpus\n",
    "from textacy.vsm import Vectorizer, GroupVectorizer\n",
    "from textacy.tm import TopicModel\n",
    "en = textacy.load_spacy(\"en_core_web_sm\", disable='parser')\n",
    "\n",
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169, 8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_directory+\"tokenized_docs.p\", \"rb\") as f:\n",
    "        docs = pkl.load(f)\n",
    "        \n",
    "with open(data_directory+\"cleaned_data.p\", \"rb\") as f:\n",
    "        orig_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(Phrases([doc[0] for doc in docs], min_count=10, threshold=20, delimiter=b' '))\n",
    "bigram_docs = [bigram_phraser[doc[0]] for doc in docs] \n",
    "\n",
    "trigram_phraser = Phraser(Phrases(bigram_docs, min_count=5, threshold=10, delimiter=b' '))\n",
    "trigram_docs = [trigram_phraser[doc] for doc in bigram_docs] \n",
    "\n",
    "analysts = [d[1]['AnalystName'] for d in docs]\n",
    "tags = [d[1]['Tag'] for d in docs]\n",
    "companies = [d[1]['Company'] for d in docs]\n",
    "quarters = [d[1]['Quarter'] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_RANK = 4\n",
    "\n",
    "a_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, analysts)\n",
    "a_doc_term_matrix = a_vec.transform(trigram_docs, analysts)\n",
    "\n",
    "a_mod = nimfa.Lsnmf(V=a_doc_term_matrix, max_iter=1000, rank=A_RANK, n_run=10)\n",
    "a_mod_fit = a_mod()\n",
    "\n",
    "a_df = pd.SparseDataFrame(normalize(a_mod_fit.basis()), columns = ['aTopic'+str(i) for i in range(A_RANK)], index=a_vec.grps_list).fillna(0)\n",
    "a_df.index.name = 'AnalystName'\n",
    "#a_df.join(a_df.idxmax(axis=1).rename('aTopicMax')).reset_index().to_csv(data_directory+\"analystTopic.csv\", index=False)\n",
    "a_df.reset_index().to_csv(data_directory+\"analystTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_RANK = 6\n",
    "\n",
    "t_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, tags)\n",
    "t_doc_term_matrix = t_vec.transform(trigram_docs, tags)\n",
    "\n",
    "t_mod = nimfa.Lsnmf(V=t_doc_term_matrix, max_iter=1000, rank=T_RANK, n_run=10)\n",
    "t_mod_fit = t_mod()\n",
    "\n",
    "t_df = pd.SparseDataFrame(normalize(t_mod_fit.basis()), columns = ['tTopic'+str(i) for i in range(T_RANK)], index=t_vec.grps_list).fillna(0)\n",
    "t_df.index.name = 'Tag'\n",
    "#t_df.join(t_df.idxmax(axis=1).rename('tTopicMax')).reset_index().to_csv(data_directory+\"tagTopic.csv\", index=False)\n",
    "t_df.reset_index().to_csv(data_directory+\"tagTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6bb8032fd281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnimfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsnmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_doc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC_RANK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mc_mod_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_mod_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cTopic'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_RANK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrps_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nimfa/models/nmf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m\"\"\"Run the specified MF algorithm.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nimfa/methods/factorization/lsnmf.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmffit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_satisfied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 c_obj = self.objective(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nimfa/methods/factorization/lsnmf.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsW\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         self.H, self.gH, self.iterH = self._subproblem(\n\u001b[0;32m--> 235\u001b[0;31m             self.V, self.W, self.H, self.epsH)\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsH\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nimfa/methods/factorization/lsnmf.py\u001b[0m in \u001b[0;36m_subproblem\u001b[0;34m(self, V, W, Hinit, epsH)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWtW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mWtV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mprojgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprojgrad\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nimfa/methods/factorization/lsnmf.py\u001b[0m in \u001b[0;36m__extract\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                 \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m                 \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m         csr_sample_values(self.shape[0], self.shape[1],\n\u001b[1;32m    349\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                           num_samples, row.ravel(), col.ravel(), val)\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# row and col are 1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "C_RANK = 4\n",
    "\n",
    "c_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, companies)\n",
    "c_doc_term_matrix = c_vec.transform(trigram_docs, companies)\n",
    "\n",
    "c_mod = nimfa.Lsnmf(V=c_doc_term_matrix, max_iter=1000, rank=C_RANK, n_run=10)\n",
    "c_mod_fit = c_mod()\n",
    "\n",
    "c_df = pd.SparseDataFrame(normalize(c_mod_fit.basis()), columns = ['cTopic'+str(i) for i in range(C_RANK)], index=c_vec.grps_list).fillna(0)\n",
    "c_df.index.name = 'Company'\n",
    "#c_df.join(c_df.idxmax(axis=1).rename('cTopicMax')).reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)\n",
    "c_df.reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_RANK = 4\n",
    "\n",
    "q_vec = GroupVectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs, quarters)\n",
    "q_doc_term_matrix = q_vec.transform(trigram_docs, quarters)\n",
    "\n",
    "q_mod = nimfa.Lsnmf(V=q_doc_term_matrix, max_iter=1000, rank=Q_RANK, n_run=10)\n",
    "q_mod_fit = q_mod()\n",
    "\n",
    "q_df = pd.SparseDataFrame(normalize(q_mod_fit.basis()), columns = ['qTopic'+str(i) for i in range(Q_RANK)], index=q_vec.grps_list).fillna(0)\n",
    "q_df.index.name = 'Quarter'\n",
    "#c_df.join(c_df.idxmax(axis=1).rename('cTopicMax')).reset_index().to_csv(data_directory+\"companyTopic.csv\", index=False)\n",
    "q_df.reset_index().to_csv(data_directory+\"quarterTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_save = ['EventNumber', 'Company', 'AnalystName', 'Month', 'Year', 'Quarter', 'EventType', 'Date'] \n",
    "\n",
    "pivot_data = pd.pivot_table(orig_data, index=cols_save, columns=['Tag'], aggfunc='size', fill_value=0)\n",
    "pivot_data.reset_index(inplace=True)\n",
    "pivot_data.columns = pivot_data.columns.get_level_values(0)\n",
    "pivot_sum_data = pivot_data.groupby(cols_save).sum(axis=1).reset_index()\n",
    "\n",
    "melt_data = pd.melt(pivot_sum_data, id_vars=cols_save, var_name=['Tag'], value_name='NumQ')\n",
    "melt_data['NumQ'] = melt_data['NumQ'].astype(bool).astype(int)\n",
    "melt_data['Tag'] = melt_data['Tag'].str.split(\"_\").str[-1]\n",
    "\n",
    "analyst_data = pd.read_csv(data_directory+\"analystTopic.csv\")\n",
    "a_topic_cols = analyst_data.drop(['AnalystName'], axis=1).columns.tolist()\n",
    "analyst_data[a_topic_cols] = analyst_data[a_topic_cols]**2\n",
    "\n",
    "tag_data = pd.read_csv(data_directory+\"tagTopic.csv\")\n",
    "t_topic_cols = tag_data.drop(['Tag'], axis=1).columns.tolist()\n",
    "tag_data[t_topic_cols] = tag_data[t_topic_cols]**2\n",
    "\n",
    "company_data = pd.read_csv(data_directory+\"companyTopic.csv\")\n",
    "c_topic_cols = company_data.drop(['Company'], axis=1).columns.tolist()\n",
    "company_data[c_topic_cols] = company_data[c_topic_cols]**2\n",
    "\n",
    "quarter_data = pd.read_csv(data_directory+\"quarterTopic.csv\")\n",
    "q_topic_cols = quarter_data.drop(['Quarter'], axis=1).columns.tolist()\n",
    "quarter_data[q_topic_cols] = quarter_data[q_topic_cols]**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merge_data = melt_data.merge(analyst_data, on=['AnalystName'])\n",
    "merge_data = merge_data.merge(tag_data, on=['Tag'])\n",
    "merge_data = merge_data.merge(company_data, on=['Company'])\n",
    "merge_data = merge_data.merge(quarter_data, on=['Quarter'])\n",
    "\n",
    "merge_data = pd.concat([merge_data,\n",
    "                      pd.get_dummies(merge_data[[\"EventType\"]])], axis=1).reset_index(drop=True)\n",
    "\n",
    "#features_data = merge_data.drop(['Company', 'AnalystName', 'QuestionNumber' ,'EventType', 'Date', 'Tag', 'Month', 'Year'], axis=1).drop_duplicates().copy()\n",
    "features_data = merge_data.drop(['Company', 'AnalystName' ,'EventType', 'Date', 'Tag', 'Month', 'Year'], axis=1).copy()\n",
    "#features_data[a_topic_cols + t_topic_cols + c_topic_cols + q_topic_cols] = features_data[a_topic_cols + t_topic_cols + c_topic_cols + q_topic_cols]**2\n",
    "train, test = features_data.loc[~features_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True), \\\n",
    "                features_data.loc[features_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ','EventNumber'], axis=1), train['NumQ']\n",
    "X_test, y_test = test.drop(['NumQ', 'EventNumber'], axis=1), test['NumQ']\n",
    "\n",
    "cols_list = train.drop(['NumQ','EventNumber'], axis=1).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7708671081105098\n",
      "Accuracy: 0.8573900148294612\n"
     ]
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, n_estimators=1000, n_iter_no_change=5).fit(X_train, y_train)\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(warm_start=True, n_estimators=1000).fit(X_train, y_train)\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))\n",
    "\n",
    "importances = estimator.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in estimator.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(indices.shape[0]):\n",
    "    \n",
    "    print(\"{}. {}: {:.4} +/- {:.4}\".format(f, cols_list[indices[f]], importances[indices[f]], std[indices[f]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, \n",
    "                                       n_estimators=1000,\n",
    "                                       learning_rate=1.0,\n",
    "                                       max_depth=25,\n",
    "                                       min_samples_leaf=25,\n",
    "                                       n_iter_no_change=5)\n",
    "\n",
    "param_grid = {'min_samples_split':[2,3,5,10,15,25]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator, \n",
    "                    param_grid,\n",
    "                    make_scorer(roc_auc_score, greater_is_better=True),\n",
    "                    cv = 5,\n",
    "                    return_train_score=False)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7725363326536347\n",
      "Accuracy: 0.8586258032624815\n"
     ]
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(warm_start=True, \n",
    "                                       n_estimators=1000,\n",
    "                                       learning_rate=0.01,\n",
    "                                       max_depth=25,\n",
    "                                       max_features='auto',\n",
    "                                       min_samples_leaf=25,\n",
    "                                       min_samples_split=10, \n",
    "                                       n_iter_no_change=5).fit(X_train, y_train)\n",
    "\n",
    "preds = estimator.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7807073669243757\n",
      "Accuracy: 0.8541769649036085\n"
     ]
    }
   ],
   "source": [
    "calibrated = CalibratedClassifierCV(estimator, method='sigmoid', cv=5).fit(X_train, y_train)\n",
    "\n",
    "preds = calibrated.predict_proba(X_test)[:,1]\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_data.drop(['NumQ', 'EventNumber'], axis=1).copy()\n",
    "y = features_data['NumQ'].copy()\n",
    "\n",
    "estimator = GradientBoostingClassifier(warm_start=True, n_estimators=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ind_data = pd.read_csv(data_directory + 'qaData.csv', parse_dates=['Date'])\n",
    "\n",
    "q_ind_data['EarningTag2'] = q_ind_data['EarningTag2'].str.strip()\n",
    "q_ind_data['Year'] = q_ind_data['Date'].dt.year\n",
    "q_ind_data['Month'] = q_ind_data['Date'].dt.month\n",
    "q_ind_data['Quarter'] = q_ind_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "q_ind_data['Company'] = q_ind_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['EventType'] = q_ind_data['EventType'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['AnalystName'] = q_ind_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "q_ind_data['Tag'] = q_ind_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "q_ind_data = q_ind_data.loc[~q_ind_data['AnalystName'].isna()].copy()\n",
    "\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(q_ind_data.groupby(['Company', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    g2.reset_index(drop=True, inplace=True)\n",
    "    g2.index.name = \"QuestionNumber\"\n",
    "    g2.reset_index(inplace=True)\n",
    "    groups.append(g2)\n",
    "\n",
    "q_ind_data = pd.concat(groups)[['EventNumber', 'QuestionNumber', 'Company', 'Month', 'Year', 'Quarter', 'EventType', 'Date', 'AnalystName', \"Tag\", \"Question\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec = Vectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(trigram_docs)\n",
    "q_doc_term_matrix = q_vec.transform(trigram_docs)\n",
    "\n",
    "t_affinity_mat = 1- sp.spatial.distance.cdist(q_doc_term_matrix.toarray(), t_doc_term_matrix.toarray(), 'cosine')\n",
    "t_affinity = pd.SparseDataFrame(t_affinity_mat, columns=t_vec.grps_list)\n",
    "\n",
    "a_affinity_mat = 1- sp.spatial.distance.cdist(q_doc_term_matrix.toarray(), a_doc_term_matrix.toarray(), 'cosine')\n",
    "a_affinity = pd.SparseDataFrame(a_affinity_mat, columns=a_vec.grps_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnalystName</th>\n",
       "      <th>aTopic0</th>\n",
       "      <th>aTopic1</th>\n",
       "      <th>aTopic2</th>\n",
       "      <th>aTopic3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdamHurwich</td>\n",
       "      <td>0.633497</td>\n",
       "      <td>0.463669</td>\n",
       "      <td>0.555154</td>\n",
       "      <td>0.274764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlevizosAlevizakos</td>\n",
       "      <td>0.500074</td>\n",
       "      <td>0.450210</td>\n",
       "      <td>0.556293</td>\n",
       "      <td>0.487622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AndrewLim</td>\n",
       "      <td>0.549875</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.465566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ArnoldKakuda</td>\n",
       "      <td>0.458182</td>\n",
       "      <td>0.368050</td>\n",
       "      <td>0.646540</td>\n",
       "      <td>0.486410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BillCarcache</td>\n",
       "      <td>0.385630</td>\n",
       "      <td>0.553890</td>\n",
       "      <td>0.414943</td>\n",
       "      <td>0.610179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AnalystName   aTopic0   aTopic1   aTopic2   aTopic3\n",
       "0         AdamHurwich  0.633497  0.463669  0.555154  0.274764\n",
       "1  AlevizosAlevizakos  0.500074  0.450210  0.556293  0.487622\n",
       "2           AndrewLim  0.549875  0.542318  0.432177  0.465566\n",
       "3        ArnoldKakuda  0.458182  0.368050  0.646540  0.486410\n",
       "4        BillCarcache  0.385630  0.553890  0.414943  0.610179"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = features_data.loc[features_data['EventNumber']==155].reset_index(drop=True).copy()\n",
    "X_event = event.drop(['NumQ', 'EventNumber'], axis=1).copy()\n",
    "y_event = event['NumQ'].copy()\n",
    "predictions = pd.Series(estimator.predict_proba(X_event)[:,1]).rename('Prediction')\n",
    "\n",
    "event = pd.concat([event, predictions], axis=1)\n",
    "event = pd.merge(event, analyst_data)\n",
    "event = pd.merge(event, tag_data)\n",
    "\n",
    "top3 = event.groupby(['AnalystName']).apply(lambda x: x[['Tag', 'Prediction']].nlargest(2, 'Prediction')).reset_index()[['AnalystName', 'Tag', 'Prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyst: BrianKlock\n",
      "\tTag: BalanceSheet (0.4571)\n",
      "\t\t(0.2655) - I want to follow-up a little bit on the deposit side, not the beta question, but on just overall balances and looking at the end-of-period spot balances. So, it looks like the DDA balances have been declining since Q3 2016 roughly, a little over $82B at the end of Q3 2016, now at $79B, and then down about 1% y-over-y. So, are you seeing commercial companies shifting into – to try to get some rate? And then, I guess, is there conversation with those customers about earnings credit or some of those deposits actually going out of the system and being used?\n",
      "\n",
      "And then, do you have the mix? The interest-bearing deposit growth kind of helped to move total deposit growth. Can you give the mix of how much of that is in CDs vs. the money market accounts?\n",
      "\tTag: Expenses (0.2109)\n",
      "\t\t(0.2388) - And I guess just as a follow-up; on the expense guide, it seems like Q4 guidance if I plug in numbers maybe it’s down $10mm or $15mm from what you’re guiding for Q3, and it seems like the FDIC surcharge, we took an estimate of something in the neighborhood of 30% to 40%, that could be benefiting your fourth quarter. So, do you guys – are you including the potential for that surcharge to go away in Q4 in your guidance, or I guess what are your thoughts – if you are including it, what are your thoughts on sort of reinvesting that savings?\n",
      "*********************************************************************\n",
      "Analyst: ChristopherKotowski\n",
      "\tTag: BalanceSheet (0.3995)\n",
      "\t\t(0.2134) - Looking at the trading action in your stock and most of the other banks, it seems to me everyone is concerned about the rising deposit betas and the flattening yield curve. But I guess when I stand back and look at it big picture I see 2.9% y-over-y loan growth and 6.9% net interest income growth. And so, clearly, you’re still getting a very significant benefit and I assume most of that is coming from the three funds – demand deposits, equity, and other float. And I guess, what needs to – when does a further Fed rate increase not become a benefit? I mean it just seems to me you’d have to have an extreme view of deposit betas or curve in order for further Fed hikes not to have a beneficial impact for you.\n",
      "\n",
      "I mean just, it’s still a benefit, right? It’s all still...\n",
      "\n",
      "The thing is it seems to me most bank kind of business models were kind of calibrated in an environment when short rates were like between 3% and 6%. I mean that was kind of historically the normal range and we’re still below that. I mean, would you say that...\n",
      "\tTag: Expenses (0.1917)\n",
      "\t\t(0.2038) - I wonder if you could give us some more color on the new private equity fund. I saw a press report saying it’s between $5B and $8B. But if you could just comment on the targeted size and mandate? I mean, is it global or North American? Is it pure corporate equity or is it broader? And then I guess as a follow-on to that, if it’s $5B to $8B, the Blackstones and Apollos of the world are raising, like, $18 billion-plus. So what’s the strategy to put the money to work? Is it to go more middle-market, to do fewer deals or to bring in more co-investors? Those kinds of decisions?\n",
      "\n",
      "Okay. And should we assume that you will commit 3% to it, and is it the standard kind of asset management, one in 20 and I assume that these will show up on the asset management side. And then kind of related to that, is this all a wealth management, asset management standalone business that’s walled off from investment banking, or can there be cross-pollination between the two?\n",
      "*********************************************************************\n",
      "Analyst: ElizabethGraseck\n",
      "\tTag: BalanceSheet (0.468)\n",
      "\t\t(0.1705) - Just a follow-up on energy question, you indicated during the prepared remarks that over 90% of the portfolio is current and that you’ve reset two times. But if prices stay where they are today, you’d have higher losses in 2016. I just wanted to understand, maybe give us a sense of as oil price comes down here, what kind of rate of change we should be expecting as we’re building up the model in 2016 for provisions?\n",
      "\n",
      "I just wanted understand how you’re thinking about that reserve because roughly 7%, it’s one of the higher ones we’ve heard of. So I’m expecting that you’re not just using the forward curve on oil to set you reserving levels.\n",
      "\n",
      "Because, I mean, you know the question has been as oil price goes down and then you get closer extraction costs and your reserves go negative, how do you deal with that kind of environment? And I’m just wondering how much of that you’ve already priced into the reserve that you’ve got.\n",
      "\tTag: Expenses (0.2325)\n",
      "\t\t(0.191) - Brian, two questions. One on the Consumer Banking efficiency ratio, you mentioned you know that there still is room for that to fall from the 52% which is obviously very efficient as it stands right now today. And you know you indicated all various opportunities to drive incremental revenues at a much improved expense ratio with all the digital that you outlined earlier. But could you speak to how the branch network could also impact those numbers? I mean your branch has been coming down about 3% the last couple of years. Is that the kind of pace that you think you're going to continue or does the digital improvements enable you to move even faster there?\n",
      "*********************************************************************\n",
      "Analyst: ErikaNajarian\n",
      "\tTag: BalanceSheet (0.4499)\n",
      "\t\t(0.2035) - So you mentioned big data. That's actually a good segue to your second priority, which is driving business innovation. We thought we would ask the audience first what they thought about investment spend and innovation before we move on to this topic. So, Eddie, if you could, please pull up the second polling question.\n",
      "\n",
      "As a shareholder, what statement most closely aligns with your view on how traditional financial institutions should allocate investment spending on innovation: one, investment spending on innovation should be top priority for financial institutions, as leading the charge in innovation will better generate long-term shareholder value rather than short-term investment expense management; two, given the revenue environment, institutions should invest in innovation projects, but be mindful of self-funding these investments with savings elsewhere in the firm; and three, given the challenging revenue environment, institutions should focus on improving the bottom line through expense management, even if this means delaying innovation projects.\n",
      "\n",
      "So we have two seconds left on the clock. So 61% of you, very much in line with your response during the BoA presentation, say that given the revenue environment, institutions should certainly focus on innovation projects, but self-fund, while 34% of you believe that investment spending in innovation should be top priority regardless of the impact to short-term expense management. Again, interestingly, Doug, only 5% say that you need to use expense management as a lever to protect that bottom line, no matter what.\n",
      "\n",
      "You can take this to the board?\n",
      "\tTag: Expenses (0.2419)\n",
      "\t\t(0.2404) - So you mentioned big data. That's actually a good segue to your second priority, which is driving business innovation. We thought we would ask the audience first what they thought about investment spend and innovation before we move on to this topic. So, Eddie, if you could, please pull up the second polling question.\n",
      "\n",
      "As a shareholder, what statement most closely aligns with your view on how traditional financial institutions should allocate investment spending on innovation: one, investment spending on innovation should be top priority for financial institutions, as leading the charge in innovation will better generate long-term shareholder value rather than short-term investment expense management; two, given the revenue environment, institutions should invest in innovation projects, but be mindful of self-funding these investments with savings elsewhere in the firm; and three, given the challenging revenue environment, institutions should focus on improving the bottom line through expense management, even if this means delaying innovation projects.\n",
      "\n",
      "So we have two seconds left on the clock. So 61% of you, very much in line with your response during the BoA presentation, say that given the revenue environment, institutions should certainly focus on innovation projects, but self-fund, while 34% of you believe that investment spending in innovation should be top priority regardless of the impact to short-term expense management. Again, interestingly, Doug, only 5% say that you need to use expense management as a lever to protect that bottom line, no matter what.\n",
      "\n",
      "You can take this to the board?\n",
      "*********************************************************************\n",
      "Analyst: GerardCassidy\n",
      "\tTag: BalanceSheet (0.4886)\n",
      "\t\t(0.171) - Okay. And then in your Community Banking metrics in slide 24, you obviously give us good data on the digital customers and such and you guys have alluded to it on the call about opening up new accounts and selling\n",
      "products for this line since that seems to be where the industry is going and you’re going. Can you share with us what kind of penetration you have, whether it’s credit cards or other types of consumer loan products, that you’re actually opening up through the online channel vs. people having to come into a branch? \n",
      "\n",
      "\n",
      "And just on the mortgage, when do you guys go live with that again? I knew it was this year, but is it first, second quarter? \n",
      "\n",
      "\n",
      "\tTag: Expenses (0.2475)\n",
      "\t\t(0.1778) - Yes. Good morning. The question is really on the compensation ratio. You've obviously done a fantastic job on the cost base, but one of the issues that strikes me is clearly with the reduced revenues, particularly in the capital markets business, your comp down about $400 million compared to the first quarter or the quarter of last year. And obviously, what we've experienced generally is you holding the ratio reasonably steady for the second to third quarter and then truing up with a lower ratio in the fourth quarter. If I'm looking at the estimates we have for revenues on a well-known provider of data, we've got pretty flat revenues being forecast by people like me whether we're right or wrong. Should we be thinking about how you're going to build a bonus pool against this background and whether we should be looking at thinking or thinking that you're going to have to retain the comp ratio at a pretty similar level through the year if indeed we don't see any uptick in revenues and they remain reasonably flat?\n",
      "\n",
      "Yeah. But I'm by no means arguing about the quality of the ratio. I'm just intrigued because clearly what we're seeing on most leaders and indeed in Europe is the difficulty of building a pool when you've been so used to having a very strong first quarter, obviously, makes it quite difficult.\n",
      "*********************************************************************\n",
      "Analyst: JohnMcdonald\n",
      "\tTag: BalanceSheet (0.468)\n",
      "\t\t(0.1692) - Good morning. Maybe just a quick question on deposit pricing. You guys have been here in the retail side have kept deposit pricing quite low. Obviously, you've done a great job in focusing on small balances, transaction and relationship-type accounts. When do you think that pressure starts to build in your business? Or is it because they're transaction you just don't think there's a lot of pressure for repricing retail deposits right now?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Right. And you're still getting good growth, so it's a good thing. Has the – if you look at the short-end\n",
      "of the curve your rate sensitivity numbers at the short-end really don't seem to have changed over the\n",
      "past year despite multiple rate hikes. Is that sort of reflective of that experience of the type of deposits\n",
      "you're attracting are lower rate than maybe you initially thought and you have a little bit more\n",
      "sensitivity at the short-end even after the number of rate hikes we've seen?\n",
      "\tTag: Expenses (0.2444)\n",
      "\t\t(0.1833) - So by far, faster incremental progress on fee revenue initiatives, and secondly, greater bottom line contribution of\n",
      "expense reduction programs. Maybe first on the expense side. You have this, what, $400 million of CIP program\n",
      "this year, two-thirds done at the half year. I guess any – last year you kind of raised all targets. Any thought in\n",
      "terms of how you think about increasing that objective?\n",
      "\n",
      "I guess in that sense and I haven't had time to add it up, but if you kind of look at a lot of these orange boxes\n",
      "between the net savings from technology infrastructure, the synergies you expect from the retail distribution\n",
      "transformation network, you kind of mentioned kind of the investment you need to make it digital. That's a lot of\n",
      "money. I mean, is all that really needed to keep digital going, or at some point do we get some of that to the\n",
      "bottom line?\n",
      "*********************************************************************\n",
      "Analyst: JohnPancari\n",
      "\tTag: BalanceSheet (0.4761)\n",
      "\t\t(0.1896) - Back to the competitive discussion and I think that Ken had brought up around commercial competition, I believe you have indicated in the past that you're staying off the fairway to agree in commercial lending around mid-market C&I and CRE given the inability to get compelling enough returns. Is this still the case and are you continuing to emphasize the specialty businesses in terms of driving growth in your commercial books?\n",
      "\n",
      "And then given that and given also what you indicated in auto right now given the competition for your loan growth expectation in the out quarters, I get what you say about next quarter but in the out quarters can you give us a little bit of color on how you're thinking about growth? Is it still something you would call or characterize as modest?\n",
      "\tTag: Expenses (0.2457)\n",
      "\t\t(0.1961) - I’ll try to be quick given the length of the call here, but I am going to beat a head horse here on the expense topic. But I know and I appreciate the color you gave on the levers that you can pull given what we’re seeing on the top line side here. Now what I’m trying to just understand is what do we need to see, maybe it’s in terms of where rates go or overall top line pressure, for you to start to really pull those levers more aggressively? And, again, maybe it’s around expenses. And also longer-term, is the goal just to keep the efficiency ratio? Are you okay with that high end of that range even through 2017? If this top line pressure persists from the curve or do you, at some point, look to get to the middle of that range again? Thanks.\n",
      "\n",
      "And then just, if rates, if we only see another 25BPS next year and that’s it, is it fair to assume that you are in that upper range still of the efficiency ratio?\n",
      "*********************************************************************\n",
      "Analyst: KennethUsdin\n",
      "\tTag: BalanceSheet (0.4585)\n",
      "\t\t(0.2003) - Hey. Can you talk a little bit about just loan growth expectations? I know, John, in your prepared remarks, you\n",
      "talked through a couple of the buckets where you’re continuing to let things roll off. But obviously, we’re seeing\n",
      "the period end is coming lower than the averages in part because of that. Can you just talk about end demand\n",
      "and try to separate end demand and customer growth from perhaps any lagging ramifications of retail sales and\n",
      "slower account growth in terms of how you expect loans to [ph] traject (43:10) going forward? Thanks.\n",
      "\n",
      "Yeah, just your general outlook for...\n",
      "\n",
      "...when do we see kind of a bottoming in loan growth and how much of that kind of let go part is weighing on the\n",
      "overall amount of growth that you’re seeing?\n",
      "\tTag: Expenses (0.2213)\n",
      "\t\t(0.1809) - Hey. Can you talk a little bit about just loan growth expectations? I know, John, in your prepared remarks, you\n",
      "talked through a couple of the buckets where you’re continuing to let things roll off. But obviously, we’re seeing\n",
      "the period end is coming lower than the averages in part because of that. Can you just talk about end demand\n",
      "and try to separate end demand and customer growth from perhaps any lagging ramifications of retail sales and\n",
      "slower account growth in terms of how you expect loans to [ph] traject (43:10) going forward? Thanks.\n",
      "\n",
      "Yeah, just your general outlook for...\n",
      "\n",
      "...when do we see kind of a bottoming in loan growth and how much of that kind of let go part is weighing on the\n",
      "overall amount of growth that you’re seeing?\n",
      "*********************************************************************\n",
      "Analyst: KevinBarker\n",
      "\tTag: BalanceSheet (0.4616)\n",
      "\t\t(0.2062) - Good morning, thanks for taking my questions. I just want to switch gears here and look at your auto exposure. You're obviously one of the largest auto lenders in the country, and you've grown that portfolio by roughly 30% over the last three years. Given what we've seen in the industry and some deterioration in the subprime portfolios, could you help us get an understanding of what your expectations are through the rest of this year and into 2017 given the state of the auto industry and how hot it's been over the last couple of years?\n",
      "\n",
      "So just to follow up on that, are you seeing better risk-adjusted returns in subprime auto versus prime auto right now, and are you growing the subprime auto portfolio at a faster rate?\n",
      "\tTag: Expenses (0.2279)\n",
      "\t\t(0.1939) - Good morning, thanks for taking my questions. I just want to switch gears here and look at your auto exposure. You're obviously one of the largest auto lenders in the country, and you've grown that portfolio by roughly 30% over the last three years. Given what we've seen in the industry and some deterioration in the subprime portfolios, could you help us get an understanding of what your expectations are through the rest of this year and into 2017 given the state of the auto industry and how hot it's been over the last couple of years?\n",
      "\n",
      "So just to follow up on that, are you seeing better risk-adjusted returns in subprime auto versus prime auto right now, and are you growing the subprime auto portfolio at a faster rate?\n",
      "*********************************************************************\n",
      "Analyst: MartyMosby\n",
      "\tTag: BalanceSheet (0.4499)\n",
      "\t\t(0.2212) - Which is good [ph] in a sense since (01:09:10) earning asset yields are moving up faster which is what we'd\n",
      "expect to happen at this particular point of the interest rate cycle as you're beginning to re-price some of the\n",
      "longer term assets as well as the short term assets, if we actually equate that to what's happening on the interestbearing deposit rates, you're only up a little bit less than 50% deposit beta this particular quarter.\n",
      "\n",
      "But if you take into account your funding which borrowings [ph] has a (01:09:40) little bit more of a fixed rate as well as non-interest-bearing funds, you really have the headroom with how far earning assets are going up at 10 or 11 basis points per quarter or per rate hike to increase interest-bearing deposit rates somewhere between 85% and 95% before you see really see any pressure on net interest margins.\n",
      "\n",
      "So it seems like there's some headroom that still see margins expand even though we're seeing deposit rates\n",
      "move up and just wanted to see what you thought about that threshold calculation.\n",
      "\n",
      "No. And I thought the highlight of transactions account growing was very important, and the only reason I was\n",
      "asking is, there's so much consternation about what margins are going to start to go down as interest rates are moving up and deposit rates are finally starting to increase. What investors I think are missing is there's a lot of headroom to still kind of trickle up those deposit rates and still continue to improve net interest margin.\n",
      "\n",
      "Thanks.\n",
      "\tTag: Expenses (0.2372)\n",
      "\t\t(0.1981) - Which is good [ph] in a sense since (01:09:10) earning asset yields are moving up faster which is what we'd\n",
      "expect to happen at this particular point of the interest rate cycle as you're beginning to re-price some of the\n",
      "longer term assets as well as the short term assets, if we actually equate that to what's happening on the interestbearing deposit rates, you're only up a little bit less than 50% deposit beta this particular quarter.\n",
      "\n",
      "But if you take into account your funding which borrowings [ph] has a (01:09:40) little bit more of a fixed rate as well as non-interest-bearing funds, you really have the headroom with how far earning assets are going up at 10 or 11 basis points per quarter or per rate hike to increase interest-bearing deposit rates somewhere between 85% and 95% before you see really see any pressure on net interest margins.\n",
      "\n",
      "So it seems like there's some headroom that still see margins expand even though we're seeing deposit rates\n",
      "move up and just wanted to see what you thought about that threshold calculation.\n",
      "\n",
      "No. And I thought the highlight of transactions account growing was very important, and the only reason I was\n",
      "asking is, there's so much consternation about what margins are going to start to go down as interest rates are moving up and deposit rates are finally starting to increase. What investors I think are missing is there's a lot of headroom to still kind of trickle up those deposit rates and still continue to improve net interest margin.\n",
      "\n",
      "Thanks.\n",
      "*********************************************************************\n",
      "Analyst: MikeMayo\n",
      "\tTag: BalanceSheet (0.4437)\n",
      "\t\t(0.1995) - My question relates to your targets and accountability to those targets. The short question is what are your targets for 2017 for ROA and ROE? And the longer version is in three parts. Number one would be the timeframe. Thank you for giving us the ROTCE target of 10% by late 2019 without any DTA write down. But from our perspective, you missed the ROE target that you had from a few years ago.\n",
      "Now you have to carry more capital, so there's a story there. But you missed the efficiency target from last January. Now you revised that in April, you made a revised target but there's a story there. And then you missed the ROA target of 90 to 110 basis points. So after missed targets this decade from our shareholder perspective, now we have to wait another two and a half years to see if you achieve your targets and it just seems like not enough accountability in the short term for 2017.\n",
      "The second question, does relate to the ROA. Can you commit to an ROA of 90 to 110 basis points in 2017? And last year in the first three minutes of this call, you highlighted how you met that target, which was good, and now in 2016 it's down to 82 basis point and you don't mention that you missed the ROA target. You don't mention it anywhere on this conference call and you don't mention it anywhere in the 89 pages that were released today. Then the third question is ROE. What is your ROE target for 2017? Again, this is down to 7.6% ROTCE. That's down from 9.2% in 2017. And after it's gotten worse, you now say excludes the DTA impact, and that would boost it to 9%. I'd say there's no credible accounting theory that would permit the exclusion of the DTA capital. If that was the case you should have taken reserve but let's go with the 9%. That's still worst in class so again, waiting two and a half years seems like a long time if we wanted to hold you accountable.\n",
      "So the timeframe, waiting another two and a half years after missed targets, the ROA target, where do you expect that to be in 2017? And what kind of ROE target can we have in 2017? And what I'm really getting at, if you were on the outside trying to hold you accountable, how would you want us to hold you accountable in 2017? What specific metrics after what shareholders have been through?\n",
      "\tTag: Expenses (0.2373)\n",
      "\t\t(0.2145) - If I can follow up, despite all the progress that you’ve made – you can about the LAS expenses, you can talk about New BAC, you can talk about the quarterly expense rate since 2011, but your core EPS is still $0.33 this quarter. And it is a weak, tough quarter, tough environment, but it’s still in that $0.35 range. So, despite all those expense savings, we’re not seeing it in the core EPS number, it’s still around $0.35. So where did all those expense savings go or is this going to come through in future quarters?\n",
      "\n",
      "I know I’ve asked this question on many earnings calls. And also I thank you Bank of America for having that, and Brian for having that opening letter from the Lead Director in the Annual Report. Jack Bovender says that he wants to engage more with investors. And so I do hope that he takes my questions at the annual meeting to engage a little bit more, because I’ve asked this so many times, I still don’t feel like I have an answer that I understand. So just one more try at it, Paul, just where are these expense savings going, if EPS is still in the same range?\n",
      "\n",
      "So no New BAC coming up or anything like that? It’s just, as I guess, Brian, you said you grind it out, it’s just – it’d be nice to see more of the progression. Is it –do you think it’s a 2016 event, 2017 event? Or we just – when run rates go up, we’ll see more of it?\n",
      "*********************************************************************\n",
      "Analyst: RobertPlacet\n",
      "\tTag: BalanceSheet (0.4009)\n",
      "\t\t(0.2945) - This is Rob from Matt's team. Just on commercial loan yields, they're up nicely this quarter. Just\n",
      "curious, how do current kind of new money yields compare just given the March rate hike, but also your\n",
      "commentary about kind of higher competition in commercial lending right now?\n",
      "\n",
      "\tTag: Expenses (0.1854)\n",
      "\t\t(0.2759) - This is Rob from Matt's team. Just on commercial loan yields, they're up nicely this quarter. Just\n",
      "curious, how do current kind of new money yields compare just given the March rate hike, but also your\n",
      "commentary about kind of higher competition in commercial lending right now?\n",
      "\n",
      "*********************************************************************\n",
      "Analyst: ScottSiefers\n",
      "\tTag: BalanceSheet (0.4604)\n",
      "\t\t(0.2101) - Hi, Scott Siefers, Sandler O'Neill. Just hoping you could dive and expand a little on the efficiency comments from earlier, obviously the efficiency ratio is not in the updated financial targets which you suggested was based on us knowing the underlying cost level. But I guess between the lack of a guide and maybe in that simulation that Saul alluded to, you know, you sort of use flattish revenues in there. I mean is the – can you just maybe expand upon your thoughts, are we going to get into that efficiency ratio it's on point in 2019 you said the existing ratio over the longer-term is still like what makes sense, but just I can sort of see the commentary gravitating toward what revenue number you guys are or not capable of hitting to back into the numbers of...\n",
      "\tTag: Expenses (0.2167)\n",
      "\t\t(0.234) - Hi, Scott Siefers, Sandler O'Neill. Just hoping you could dive and expand a little on the efficiency comments from earlier, obviously the efficiency ratio is not in the updated financial targets which you suggested was based on us knowing the underlying cost level. But I guess between the lack of a guide and maybe in that simulation that Saul alluded to, you know, you sort of use flattish revenues in there. I mean is the – can you just maybe expand upon your thoughts, are we going to get into that efficiency ratio it's on point in 2019 you said the existing ratio over the longer-term is still like what makes sense, but just I can sort of see the commentary gravitating toward what revenue number you guys are or not capable of hitting to back into the numbers of...\n",
      "*********************************************************************\n"
     ]
    }
   ],
   "source": [
    "pred_q = {}\n",
    "\n",
    "for ind in top3[['AnalystName', 'Tag']].values:\n",
    "    a, t = ind\n",
    "    if a not in pred_q:\n",
    "        pred_q[a] = {}\n",
    "    \n",
    "    pred_q[a][t] = []\n",
    "    affinities = (a_affinity[a] + t_affinity[t])/2\n",
    "    q_ind = affinities.nlargest(1).reset_index()['index'].values\n",
    "\n",
    "    for val in q_ind:\n",
    "        #' '.join(docs[val][0])\n",
    "        question = q_ind_data.loc[(q_ind_data['EventNumber']==docs[val][1]['EventNumber']) & \n",
    "                       (q_ind_data['QuestionNumber']==docs[val][1]['QuestionNumber']),'Question'].item()\n",
    "        pred_q[a][t].append((question, affinities[val]))\n",
    "\n",
    "for a, t_dict in pred_q.items():\n",
    "    print(\"Analyst: {}\".format(a))\n",
    "    for t, values in pred_q[a].items():\n",
    "        prob = top3.loc[(top3['AnalystName']==a) & (top3['Tag']==t),'Prediction'].item()\n",
    "        print(\"\\tTag: {} ({:.4})\".format(t, prob))\n",
    "        for v0, v1 in values:\n",
    "            print(\"\\t\\t({:.4}) - {}\".format(v1, v0))\n",
    "    print(\"*********************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
