{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle as plk\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/luyin/Desktop/project/Q&A.xlsx',header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df['Analyst name'].unique() # 79 unique analyst\n",
    "dic = {} #create dictionary for questions\n",
    "\n",
    "for name in l:\n",
    "    list_ = list(df.loc[df['Analyst name']  == name]['Question'])\n",
    "    dic[name] = list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS |= {'\\xa0','\\n', \" \"}\n",
    "# STOP_WORDS -= {'who','where','how', 'what', 'when', 'whenever'}\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "def tokenize(sent):\n",
    "#     sent = re.sub('[^A-Za-z&]', ' ', sent) # replace non-letter with space\n",
    "    sent = re.sub('^[0-9]+', '', sent)\n",
    "#     sent = re.sub(r'\\b[a-zA-Z]\\b', '', sent) #remove single letter \n",
    "    tokens = tokenizer(sent)\n",
    "    return [(token.text.lower()) for token in tokens if (token.text.lower() not in punctuations and token.is_alpha and token.text.lower() not in {'\\xa0', ' ',\" \"})]\n",
    "\n",
    "#     return [(token.lemma_) for token in tokens if \\\n",
    "#             (token.text.lower() not in punctuations \\\n",
    "#              and token.lemma_ not in STOP_WORDS \\\n",
    "#              and token.pos_ in ('PROPN', 'VERB', 'NOUN'))] # only keep PRONOUN, VERB and NOUN\n",
    "# # test\n",
    "# tokens = tokenize(\" going to hit them one way or another strong dollar did seem to have a huge impact and y ou, what are you doing? I'm 's what do you and me think or like apples and Apple is looking at buying and bought U.K. startup for $1 billion. '\\n' another sentence\")\n",
    "# for token in tokens:\n",
    "#     print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_analyst_q(name):\n",
    "    all_tokens = []\n",
    "    for q in dic[name]:\n",
    "        tokens = tokenize(q)\n",
    "        all_tokens += tokens\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_analyst_q('Glenn Schorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12024"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(input_):\n",
    "    vocab = sorted(set(input_))\n",
    "    vocab_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "    int_to_vocab = dict((i, c) for i, c in enumerate(vocab))\n",
    "    n_total = len(input_)\n",
    "    n_vocab = len(vocab)\n",
    "    print (\"Total Words: \",n_total)\n",
    "    print (\"Total Vocab: \",n_vocab)\n",
    "    return n_total, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "def build_sequence(length, input_):\n",
    "    sequences = list()\n",
    "    for i in range(length, len(input_)):\n",
    "    # select sequence of tokens\n",
    "        seq = input_[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  12024\n",
      "Total Vocab:  1615\n",
      "Total Sequences: 12004\n"
     ]
    }
   ],
   "source": [
    "n_total, n_vocab = build_vocab(tokens)\n",
    "sequences = build_sequence(20, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'Glenn_Schorr_sequences.txt'\n",
    "save_doc(sequences, out_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 19, 50)            80800     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1616)              163216    \n",
      "=================================================================\n",
      "Total params: 394,916\n",
      "Trainable params: 394,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "12004/12004 [==============================] - 17s 1ms/step - loss: 6.4232 - acc: 0.0408\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.42324, saving model to LSTM_basline.hdf5\n",
      "Epoch 2/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 5.8626 - acc: 0.0548\n",
      "\n",
      "Epoch 00002: loss improved from 6.42324 to 5.86261, saving model to LSTM_basline.hdf5\n",
      "Epoch 3/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.8423 - acc: 0.0548\n",
      "\n",
      "Epoch 00003: loss improved from 5.86261 to 5.84231, saving model to LSTM_basline.hdf5\n",
      "Epoch 4/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.8378 - acc: 0.0548\n",
      "\n",
      "Epoch 00004: loss improved from 5.84231 to 5.83783, saving model to LSTM_basline.hdf5\n",
      "Epoch 5/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.8255 - acc: 0.0548\n",
      "\n",
      "Epoch 00005: loss improved from 5.83783 to 5.82549, saving model to LSTM_basline.hdf5\n",
      "Epoch 6/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.7533 - acc: 0.0548\n",
      "\n",
      "Epoch 00006: loss improved from 5.82549 to 5.75330, saving model to LSTM_basline.hdf5\n",
      "Epoch 7/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.6636 - acc: 0.0549\n",
      "\n",
      "Epoch 00007: loss improved from 5.75330 to 5.66365, saving model to LSTM_basline.hdf5\n",
      "Epoch 8/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.5679 - acc: 0.0542\n",
      "\n",
      "Epoch 00008: loss improved from 5.66365 to 5.56792, saving model to LSTM_basline.hdf5\n",
      "Epoch 9/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.4949 - acc: 0.0595\n",
      "\n",
      "Epoch 00009: loss improved from 5.56792 to 5.49487, saving model to LSTM_basline.hdf5\n",
      "Epoch 10/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.4458 - acc: 0.0648\n",
      "\n",
      "Epoch 00010: loss improved from 5.49487 to 5.44575, saving model to LSTM_basline.hdf5\n",
      "Epoch 11/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.3819 - acc: 0.0693\n",
      "\n",
      "Epoch 00011: loss improved from 5.44575 to 5.38185, saving model to LSTM_basline.hdf5\n",
      "Epoch 12/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.3210 - acc: 0.0712: 1s - loss: 5.3215 - ac\n",
      "\n",
      "Epoch 00012: loss improved from 5.38185 to 5.32101, saving model to LSTM_basline.hdf5\n",
      "Epoch 13/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.2674 - acc: 0.0737\n",
      "\n",
      "Epoch 00013: loss improved from 5.32101 to 5.26742, saving model to LSTM_basline.hdf5\n",
      "Epoch 14/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.2312 - acc: 0.0765\n",
      "\n",
      "Epoch 00014: loss improved from 5.26742 to 5.23123, saving model to LSTM_basline.hdf5\n",
      "Epoch 15/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.1944 - acc: 0.0780\n",
      "\n",
      "Epoch 00015: loss improved from 5.23123 to 5.19435, saving model to LSTM_basline.hdf5\n",
      "Epoch 16/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.1589 - acc: 0.0822\n",
      "\n",
      "Epoch 00016: loss improved from 5.19435 to 5.15889, saving model to LSTM_basline.hdf5\n",
      "Epoch 17/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.1286 - acc: 0.0823\n",
      "\n",
      "Epoch 00017: loss improved from 5.15889 to 5.12859, saving model to LSTM_basline.hdf5\n",
      "Epoch 18/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.1052 - acc: 0.0821\n",
      "\n",
      "Epoch 00018: loss improved from 5.12859 to 5.10523, saving model to LSTM_basline.hdf5\n",
      "Epoch 19/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 5.0813 - acc: 0.0860\n",
      "\n",
      "Epoch 00019: loss improved from 5.10523 to 5.08126, saving model to LSTM_basline.hdf5\n",
      "Epoch 20/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.0561 - acc: 0.0855\n",
      "\n",
      "Epoch 00020: loss improved from 5.08126 to 5.05612, saving model to LSTM_basline.hdf5\n",
      "Epoch 21/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.0350 - acc: 0.0873\n",
      "\n",
      "Epoch 00021: loss improved from 5.05612 to 5.03500, saving model to LSTM_basline.hdf5\n",
      "Epoch 22/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 5.0029 - acc: 0.0886: 5s -\n",
      "\n",
      "Epoch 00022: loss improved from 5.03500 to 5.00289, saving model to LSTM_basline.hdf5\n",
      "Epoch 23/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.9810 - acc: 0.0923\n",
      "\n",
      "Epoch 00023: loss improved from 5.00289 to 4.98104, saving model to LSTM_basline.hdf5\n",
      "Epoch 24/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.9487 - acc: 0.0922\n",
      "\n",
      "Epoch 00024: loss improved from 4.98104 to 4.94869, saving model to LSTM_basline.hdf5\n",
      "Epoch 25/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.9124 - acc: 0.0956\n",
      "\n",
      "Epoch 00025: loss improved from 4.94869 to 4.91238, saving model to LSTM_basline.hdf5\n",
      "Epoch 26/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.8744 - acc: 0.1000\n",
      "\n",
      "Epoch 00026: loss improved from 4.91238 to 4.87443, saving model to LSTM_basline.hdf5\n",
      "Epoch 27/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.8422 - acc: 0.1045\n",
      "\n",
      "Epoch 00027: loss improved from 4.87443 to 4.84223, saving model to LSTM_basline.hdf5\n",
      "Epoch 28/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.8021 - acc: 0.1076\n",
      "\n",
      "Epoch 00028: loss improved from 4.84223 to 4.80210, saving model to LSTM_basline.hdf5\n",
      "Epoch 29/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.7662 - acc: 0.1138\n",
      "\n",
      "Epoch 00029: loss improved from 4.80210 to 4.76619, saving model to LSTM_basline.hdf5\n",
      "Epoch 30/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.7322 - acc: 0.1168\n",
      "\n",
      "Epoch 00030: loss improved from 4.76619 to 4.73225, saving model to LSTM_basline.hdf5\n",
      "Epoch 31/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.6914 - acc: 0.1216\n",
      "\n",
      "Epoch 00031: loss improved from 4.73225 to 4.69143, saving model to LSTM_basline.hdf5\n",
      "Epoch 32/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.6599 - acc: 0.1207\n",
      "\n",
      "Epoch 00032: loss improved from 4.69143 to 4.65990, saving model to LSTM_basline.hdf5\n",
      "Epoch 33/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.6353 - acc: 0.1262\n",
      "\n",
      "Epoch 00033: loss improved from 4.65990 to 4.63526, saving model to LSTM_basline.hdf5\n",
      "Epoch 34/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.5969 - acc: 0.1274\n",
      "\n",
      "Epoch 00034: loss improved from 4.63526 to 4.59689, saving model to LSTM_basline.hdf5\n",
      "Epoch 35/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.5700 - acc: 0.1340\n",
      "\n",
      "Epoch 00035: loss improved from 4.59689 to 4.57004, saving model to LSTM_basline.hdf5\n",
      "Epoch 36/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.5378 - acc: 0.1346\n",
      "\n",
      "Epoch 00036: loss improved from 4.57004 to 4.53784, saving model to LSTM_basline.hdf5\n",
      "Epoch 37/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.5118 - acc: 0.1424\n",
      "\n",
      "Epoch 00037: loss improved from 4.53784 to 4.51178, saving model to LSTM_basline.hdf5\n",
      "Epoch 38/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.4778 - acc: 0.1428\n",
      "\n",
      "Epoch 00038: loss improved from 4.51178 to 4.47781, saving model to LSTM_basline.hdf5\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.4519 - acc: 0.1426\n",
      "\n",
      "Epoch 00039: loss improved from 4.47781 to 4.45190, saving model to LSTM_basline.hdf5\n",
      "Epoch 40/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.4240 - acc: 0.1478\n",
      "\n",
      "Epoch 00040: loss improved from 4.45190 to 4.42399, saving model to LSTM_basline.hdf5\n",
      "Epoch 41/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.4016 - acc: 0.1465\n",
      "\n",
      "Epoch 00041: loss improved from 4.42399 to 4.40159, saving model to LSTM_basline.hdf5\n",
      "Epoch 42/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.3858 - acc: 0.1497\n",
      "\n",
      "Epoch 00042: loss improved from 4.40159 to 4.38577, saving model to LSTM_basline.hdf5\n",
      "Epoch 43/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.3534 - acc: 0.1519\n",
      "\n",
      "Epoch 00043: loss improved from 4.38577 to 4.35336, saving model to LSTM_basline.hdf5\n",
      "Epoch 44/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.3316 - acc: 0.1534\n",
      "\n",
      "Epoch 00044: loss improved from 4.35336 to 4.33163, saving model to LSTM_basline.hdf5\n",
      "Epoch 45/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.3071 - acc: 0.1542\n",
      "\n",
      "Epoch 00045: loss improved from 4.33163 to 4.30714, saving model to LSTM_basline.hdf5\n",
      "Epoch 46/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.2948 - acc: 0.1569\n",
      "\n",
      "Epoch 00046: loss improved from 4.30714 to 4.29479, saving model to LSTM_basline.hdf5\n",
      "Epoch 47/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.2676 - acc: 0.1573\n",
      "\n",
      "Epoch 00047: loss improved from 4.29479 to 4.26759, saving model to LSTM_basline.hdf5\n",
      "Epoch 48/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.2407 - acc: 0.1574\n",
      "\n",
      "Epoch 00048: loss improved from 4.26759 to 4.24067, saving model to LSTM_basline.hdf5\n",
      "Epoch 49/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.2108 - acc: 0.1613\n",
      "\n",
      "Epoch 00049: loss improved from 4.24067 to 4.21078, saving model to LSTM_basline.hdf5\n",
      "Epoch 50/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.2049 - acc: 0.1611\n",
      "\n",
      "Epoch 00050: loss improved from 4.21078 to 4.20489, saving model to LSTM_basline.hdf5\n",
      "Epoch 51/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.1803 - acc: 0.1629\n",
      "\n",
      "Epoch 00051: loss improved from 4.20489 to 4.18031, saving model to LSTM_basline.hdf5\n",
      "Epoch 52/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.1714 - acc: 0.1623\n",
      "\n",
      "Epoch 00052: loss improved from 4.18031 to 4.17142, saving model to LSTM_basline.hdf5\n",
      "Epoch 53/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.1368 - acc: 0.1653\n",
      "\n",
      "Epoch 00053: loss improved from 4.17142 to 4.13677, saving model to LSTM_basline.hdf5\n",
      "Epoch 54/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.1484 - acc: 0.1616\n",
      "\n",
      "Epoch 00054: loss did not improve from 4.13677\n",
      "Epoch 55/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.1038 - acc: 0.1649\n",
      "\n",
      "Epoch 00055: loss improved from 4.13677 to 4.10380, saving model to LSTM_basline.hdf5\n",
      "Epoch 56/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.0659 - acc: 0.1729\n",
      "\n",
      "Epoch 00056: loss improved from 4.10380 to 4.06594, saving model to LSTM_basline.hdf5\n",
      "Epoch 57/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.0498 - acc: 0.1699\n",
      "\n",
      "Epoch 00057: loss improved from 4.06594 to 4.04983, saving model to LSTM_basline.hdf5\n",
      "Epoch 58/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 4.0237 - acc: 0.1743\n",
      "\n",
      "Epoch 00058: loss improved from 4.04983 to 4.02371, saving model to LSTM_basline.hdf5\n",
      "Epoch 59/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 4.0025 - acc: 0.1751\n",
      "\n",
      "Epoch 00059: loss improved from 4.02371 to 4.00250, saving model to LSTM_basline.hdf5\n",
      "Epoch 60/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.9873 - acc: 0.1764\n",
      "\n",
      "Epoch 00060: loss improved from 4.00250 to 3.98733, saving model to LSTM_basline.hdf5\n",
      "Epoch 61/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.9626 - acc: 0.1754\n",
      "\n",
      "Epoch 00061: loss improved from 3.98733 to 3.96265, saving model to LSTM_basline.hdf5\n",
      "Epoch 62/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.9433 - acc: 0.1812\n",
      "\n",
      "Epoch 00062: loss improved from 3.96265 to 3.94327, saving model to LSTM_basline.hdf5\n",
      "Epoch 63/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.9229 - acc: 0.1802\n",
      "\n",
      "Epoch 00063: loss improved from 3.94327 to 3.92288, saving model to LSTM_basline.hdf5\n",
      "Epoch 64/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.9164 - acc: 0.1782\n",
      "\n",
      "Epoch 00064: loss improved from 3.92288 to 3.91635, saving model to LSTM_basline.hdf5\n",
      "Epoch 65/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.8860 - acc: 0.1864\n",
      "\n",
      "Epoch 00065: loss improved from 3.91635 to 3.88596, saving model to LSTM_basline.hdf5\n",
      "Epoch 66/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.8576 - acc: 0.1858\n",
      "\n",
      "Epoch 00066: loss improved from 3.88596 to 3.85759, saving model to LSTM_basline.hdf5\n",
      "Epoch 67/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.8446 - acc: 0.1891\n",
      "\n",
      "Epoch 00067: loss improved from 3.85759 to 3.84463, saving model to LSTM_basline.hdf5\n",
      "Epoch 68/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.8241 - acc: 0.1915\n",
      "\n",
      "Epoch 00068: loss improved from 3.84463 to 3.82410, saving model to LSTM_basline.hdf5\n",
      "Epoch 69/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.8026 - acc: 0.1931\n",
      "\n",
      "Epoch 00069: loss improved from 3.82410 to 3.80260, saving model to LSTM_basline.hdf5\n",
      "Epoch 70/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.7869 - acc: 0.1940\n",
      "\n",
      "Epoch 00070: loss improved from 3.80260 to 3.78690, saving model to LSTM_basline.hdf5\n",
      "Epoch 71/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.7628 - acc: 0.2006\n",
      "\n",
      "Epoch 00071: loss improved from 3.78690 to 3.76280, saving model to LSTM_basline.hdf5\n",
      "Epoch 72/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.7427 - acc: 0.2003\n",
      "\n",
      "Epoch 00072: loss improved from 3.76280 to 3.74267, saving model to LSTM_basline.hdf5\n",
      "Epoch 73/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.7283 - acc: 0.2020\n",
      "\n",
      "Epoch 00073: loss improved from 3.74267 to 3.72825, saving model to LSTM_basline.hdf5\n",
      "Epoch 74/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.7183 - acc: 0.2003\n",
      "\n",
      "Epoch 00074: loss improved from 3.72825 to 3.71830, saving model to LSTM_basline.hdf5\n",
      "Epoch 75/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.6906 - acc: 0.2032\n",
      "\n",
      "Epoch 00075: loss improved from 3.71830 to 3.69060, saving model to LSTM_basline.hdf5\n",
      "Epoch 76/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.6658 - acc: 0.2114\n",
      "\n",
      "Epoch 00076: loss improved from 3.69060 to 3.66575, saving model to LSTM_basline.hdf5\n",
      "Epoch 77/100\n",
      "12004/12004 [==============================] - 151s 13ms/step - loss: 3.6469 - acc: 0.2099\n",
      "\n",
      "Epoch 00077: loss improved from 3.66575 to 3.64687, saving model to LSTM_basline.hdf5\n",
      "Epoch 78/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.6413 - acc: 0.2084\n",
      "\n",
      "Epoch 00078: loss improved from 3.64687 to 3.64128, saving model to LSTM_basline.hdf5\n",
      "Epoch 79/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.6224 - acc: 0.2106\n",
      "\n",
      "Epoch 00079: loss improved from 3.64128 to 3.62237, saving model to LSTM_basline.hdf5\n",
      "Epoch 80/100\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.6012 - acc: 0.2132\n",
      "\n",
      "Epoch 00080: loss improved from 3.62237 to 3.60124, saving model to LSTM_basline.hdf5\n",
      "Epoch 81/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.5884 - acc: 0.2118\n",
      "\n",
      "Epoch 00081: loss improved from 3.60124 to 3.58841, saving model to LSTM_basline.hdf5\n",
      "Epoch 82/100\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.5622 - acc: 0.2218\n",
      "\n",
      "Epoch 00082: loss improved from 3.58841 to 3.56217, saving model to LSTM_basline.hdf5\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.5426 - acc: 0.2213\n",
      "\n",
      "Epoch 00083: loss improved from 3.56217 to 3.54263, saving model to LSTM_basline.hdf5\n",
      "Epoch 84/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.5284 - acc: 0.2266\n",
      "\n",
      "Epoch 00084: loss improved from 3.54263 to 3.52840, saving model to LSTM_basline.hdf5\n",
      "Epoch 85/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.5021 - acc: 0.2283\n",
      "\n",
      "Epoch 00085: loss improved from 3.52840 to 3.50213, saving model to LSTM_basline.hdf5\n",
      "Epoch 86/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.4879 - acc: 0.2298\n",
      "\n",
      "Epoch 00086: loss improved from 3.50213 to 3.48795, saving model to LSTM_basline.hdf5\n",
      "Epoch 87/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.4782 - acc: 0.2304\n",
      "\n",
      "Epoch 00087: loss improved from 3.48795 to 3.47823, saving model to LSTM_basline.hdf5\n",
      "Epoch 88/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.4537 - acc: 0.2308\n",
      "\n",
      "Epoch 00088: loss improved from 3.47823 to 3.45366, saving model to LSTM_basline.hdf5\n",
      "Epoch 89/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.4496 - acc: 0.2346\n",
      "\n",
      "Epoch 00089: loss improved from 3.45366 to 3.44964, saving model to LSTM_basline.hdf5\n",
      "Epoch 90/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.4171 - acc: 0.2373\n",
      "\n",
      "Epoch 00090: loss improved from 3.44964 to 3.41707, saving model to LSTM_basline.hdf5\n",
      "Epoch 91/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.4198 - acc: 0.2391\n",
      "\n",
      "Epoch 00091: loss did not improve from 3.41707\n",
      "Epoch 92/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3885 - acc: 0.2447\n",
      "\n",
      "Epoch 00092: loss improved from 3.41707 to 3.38846, saving model to LSTM_basline.hdf5\n",
      "Epoch 93/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3750 - acc: 0.2451\n",
      "\n",
      "Epoch 00093: loss improved from 3.38846 to 3.37503, saving model to LSTM_basline.hdf5\n",
      "Epoch 94/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3538 - acc: 0.2458\n",
      "\n",
      "Epoch 00094: loss improved from 3.37503 to 3.35381, saving model to LSTM_basline.hdf5\n",
      "Epoch 95/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3295 - acc: 0.2511\n",
      "\n",
      "Epoch 00095: loss improved from 3.35381 to 3.32948, saving model to LSTM_basline.hdf5\n",
      "Epoch 96/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3218 - acc: 0.2480\n",
      "\n",
      "Epoch 00096: loss improved from 3.32948 to 3.32183, saving model to LSTM_basline.hdf5\n",
      "Epoch 97/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.3092 - acc: 0.2499\n",
      "\n",
      "Epoch 00097: loss improved from 3.32183 to 3.30924, saving model to LSTM_basline.hdf5\n",
      "Epoch 98/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2744 - acc: 0.2589\n",
      "\n",
      "Epoch 00098: loss improved from 3.30924 to 3.27441, saving model to LSTM_basline.hdf5\n",
      "Epoch 99/100\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.2559 - acc: 0.2584\n",
      "\n",
      "Epoch 00099: loss improved from 3.27441 to 3.25588, saving model to LSTM_basline.hdf5\n",
      "Epoch 100/100\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2639 - acc: 0.2586\n",
      "\n",
      "Epoch 00100: loss did not improve from 3.25588\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "\n",
    "filepath=\"LSTM_basline.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=200, epochs=100, callbacks=callbacks_list)\n",
    "\n",
    "# categorical_crossentropy\n",
    "# https://cloud.githubusercontent.com/assets/18217467/25556005/1bb51402-2d12-11e7-87ef-a9bad097a858.png\n",
    " \n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "plk.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutely awesome i do have one tiny follow up i always get a little more than i wanted there thank\n",
      "\n",
      "that of the past is i just curious if you could give the question but i guess the question but i just curious if you could give the question but i guess the question but i just curious if you could give the question but i guess the question but\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "    # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12004/12004 [==============================] - 18s 2ms/step - loss: 3.3733 - acc: 0.2468\n",
      "\n",
      "Epoch 00001: loss did not improve from 3.25588\n",
      "Epoch 2/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2386 - acc: 0.2646\n",
      "\n",
      "Epoch 00002: loss improved from 3.25588 to 3.23858, saving model to LSTM_basline.hdf5\n",
      "Epoch 3/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.2111 - acc: 0.2687\n",
      "\n",
      "Epoch 00003: loss improved from 3.23858 to 3.21107, saving model to LSTM_basline.hdf5\n",
      "Epoch 4/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1937 - acc: 0.2726\n",
      "\n",
      "Epoch 00004: loss improved from 3.21107 to 3.19372, saving model to LSTM_basline.hdf5\n",
      "Epoch 5/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1613 - acc: 0.2755\n",
      "\n",
      "Epoch 00005: loss improved from 3.19372 to 3.16126, saving model to LSTM_basline.hdf5\n",
      "Epoch 6/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1517 - acc: 0.2795\n",
      "\n",
      "Epoch 00006: loss improved from 3.16126 to 3.15166, saving model to LSTM_basline.hdf5\n",
      "Epoch 7/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1368 - acc: 0.2837\n",
      "\n",
      "Epoch 00007: loss improved from 3.15166 to 3.13680, saving model to LSTM_basline.hdf5\n",
      "Epoch 8/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1277 - acc: 0.2837\n",
      "\n",
      "Epoch 00008: loss improved from 3.13680 to 3.12772, saving model to LSTM_basline.hdf5\n",
      "Epoch 9/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.1003 - acc: 0.2884\n",
      "\n",
      "Epoch 00009: loss improved from 3.12772 to 3.10034, saving model to LSTM_basline.hdf5\n",
      "Epoch 10/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.0912 - acc: 0.2872\n",
      "\n",
      "Epoch 00010: loss improved from 3.10034 to 3.09116, saving model to LSTM_basline.hdf5\n",
      "Epoch 11/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.1042 - acc: 0.2807\n",
      "\n",
      "Epoch 00011: loss did not improve from 3.09116\n",
      "Epoch 12/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.0543 - acc: 0.2919\n",
      "\n",
      "Epoch 00012: loss improved from 3.09116 to 3.05433, saving model to LSTM_basline.hdf5\n",
      "Epoch 13/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 3.0469 - acc: 0.2962\n",
      "\n",
      "Epoch 00013: loss improved from 3.05433 to 3.04688, saving model to LSTM_basline.hdf5\n",
      "Epoch 14/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.0372 - acc: 0.2974\n",
      "\n",
      "Epoch 00014: loss improved from 3.04688 to 3.03725, saving model to LSTM_basline.hdf5\n",
      "Epoch 15/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 3.0147 - acc: 0.3001\n",
      "\n",
      "Epoch 00015: loss improved from 3.03725 to 3.01472, saving model to LSTM_basline.hdf5\n",
      "Epoch 16/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9963 - acc: 0.3071\n",
      "\n",
      "Epoch 00016: loss improved from 3.01472 to 2.99628, saving model to LSTM_basline.hdf5\n",
      "Epoch 17/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 3.0016 - acc: 0.3018\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.99628\n",
      "Epoch 18/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9748 - acc: 0.3065\n",
      "\n",
      "Epoch 00018: loss improved from 2.99628 to 2.97476, saving model to LSTM_basline.hdf5\n",
      "Epoch 19/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9606 - acc: 0.3101\n",
      "\n",
      "Epoch 00019: loss improved from 2.97476 to 2.96064, saving model to LSTM_basline.hdf5\n",
      "Epoch 20/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9430 - acc: 0.3168\n",
      "\n",
      "Epoch 00020: loss improved from 2.96064 to 2.94305, saving model to LSTM_basline.hdf5\n",
      "Epoch 21/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.9430 - acc: 0.3156\n",
      "\n",
      "Epoch 00021: loss improved from 2.94305 to 2.94303, saving model to LSTM_basline.hdf5\n",
      "Epoch 22/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.9247 - acc: 0.3151\n",
      "\n",
      "Epoch 00022: loss improved from 2.94303 to 2.92467, saving model to LSTM_basline.hdf5\n",
      "Epoch 23/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.9150 - acc: 0.3166\n",
      "\n",
      "Epoch 00023: loss improved from 2.92467 to 2.91505, saving model to LSTM_basline.hdf5\n",
      "Epoch 24/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.9156 - acc: 0.3174\n",
      "\n",
      "Epoch 00024: loss did not improve from 2.91505\n",
      "Epoch 25/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8777 - acc: 0.3301\n",
      "\n",
      "Epoch 00025: loss improved from 2.91505 to 2.87766, saving model to LSTM_basline.hdf5\n",
      "Epoch 26/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8804 - acc: 0.3246\n",
      "\n",
      "Epoch 00026: loss did not improve from 2.87766\n",
      "Epoch 27/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8787 - acc: 0.3237\n",
      "\n",
      "Epoch 00027: loss did not improve from 2.87766\n",
      "Epoch 28/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.8493 - acc: 0.3332\n",
      "\n",
      "Epoch 00028: loss improved from 2.87766 to 2.84933, saving model to LSTM_basline.hdf5\n",
      "Epoch 29/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8379 - acc: 0.3364\n",
      "\n",
      "Epoch 00029: loss improved from 2.84933 to 2.83791, saving model to LSTM_basline.hdf5\n",
      "Epoch 30/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.8195 - acc: 0.3381\n",
      "\n",
      "Epoch 00030: loss improved from 2.83791 to 2.81951, saving model to LSTM_basline.hdf5\n",
      "Epoch 31/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8242 - acc: 0.3333\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.81951\n",
      "Epoch 32/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.8089 - acc: 0.3355\n",
      "\n",
      "Epoch 00032: loss improved from 2.81951 to 2.80893, saving model to LSTM_basline.hdf5\n",
      "Epoch 33/500\n",
      "12004/12004 [==============================] - 12s 1ms/step - loss: 2.7941 - acc: 0.3436\n",
      "\n",
      "Epoch 00033: loss improved from 2.80893 to 2.79409, saving model to LSTM_basline.hdf5\n",
      "Epoch 34/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.8166 - acc: 0.3373\n",
      "\n",
      "Epoch 00034: loss did not improve from 2.79409\n",
      "Epoch 35/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7765 - acc: 0.3456\n",
      "\n",
      "Epoch 00035: loss improved from 2.79409 to 2.77651, saving model to LSTM_basline.hdf5\n",
      "Epoch 36/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7757 - acc: 0.3468\n",
      "\n",
      "Epoch 00036: loss improved from 2.77651 to 2.77566, saving model to LSTM_basline.hdf5\n",
      "Epoch 37/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7734 - acc: 0.3438\n",
      "\n",
      "Epoch 00037: loss improved from 2.77566 to 2.77344, saving model to LSTM_basline.hdf5\n",
      "Epoch 38/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7388 - acc: 0.3517\n",
      "\n",
      "Epoch 00038: loss improved from 2.77344 to 2.73880, saving model to LSTM_basline.hdf5\n",
      "Epoch 39/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7253 - acc: 0.3555\n",
      "\n",
      "Epoch 00039: loss improved from 2.73880 to 2.72531, saving model to LSTM_basline.hdf5\n",
      "Epoch 40/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.7271 - acc: 0.3551\n",
      "\n",
      "Epoch 00040: loss did not improve from 2.72531\n",
      "Epoch 41/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.7079 - acc: 0.3581\n",
      "\n",
      "Epoch 00041: loss improved from 2.72531 to 2.70788, saving model to LSTM_basline.hdf5\n",
      "Epoch 42/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6877 - acc: 0.3598\n",
      "\n",
      "Epoch 00042: loss improved from 2.70788 to 2.68774, saving model to LSTM_basline.hdf5\n",
      "Epoch 43/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6949 - acc: 0.3582\n",
      "\n",
      "Epoch 00043: loss did not improve from 2.68774\n",
      "Epoch 44/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.7018 - acc: 0.3564\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.68774\n",
      "Epoch 45/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6838 - acc: 0.3615\n",
      "\n",
      "Epoch 00045: loss improved from 2.68774 to 2.68375, saving model to LSTM_basline.hdf5\n",
      "Epoch 46/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6612 - acc: 0.3691\n",
      "\n",
      "Epoch 00046: loss improved from 2.68375 to 2.66119, saving model to LSTM_basline.hdf5\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6505 - acc: 0.3688\n",
      "\n",
      "Epoch 00047: loss improved from 2.66119 to 2.65046, saving model to LSTM_basline.hdf5\n",
      "Epoch 48/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.6528 - acc: 0.3655\n",
      "\n",
      "Epoch 00048: loss did not improve from 2.65046\n",
      "Epoch 49/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6265 - acc: 0.3700\n",
      "\n",
      "Epoch 00049: loss improved from 2.65046 to 2.62651, saving model to LSTM_basline.hdf5\n",
      "Epoch 50/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6029 - acc: 0.3795\n",
      "\n",
      "Epoch 00050: loss improved from 2.62651 to 2.60288, saving model to LSTM_basline.hdf5\n",
      "Epoch 51/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6177 - acc: 0.3751\n",
      "\n",
      "Epoch 00051: loss did not improve from 2.60288\n",
      "Epoch 52/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.6046 - acc: 0.3744\n",
      "\n",
      "Epoch 00052: loss did not improve from 2.60288\n",
      "Epoch 53/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5947 - acc: 0.3798\n",
      "\n",
      "Epoch 00053: loss improved from 2.60288 to 2.59475, saving model to LSTM_basline.hdf5\n",
      "Epoch 54/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5941 - acc: 0.3804\n",
      "\n",
      "Epoch 00054: loss improved from 2.59475 to 2.59408, saving model to LSTM_basline.hdf5\n",
      "Epoch 55/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5795 - acc: 0.3839\n",
      "\n",
      "Epoch 00055: loss improved from 2.59408 to 2.57952, saving model to LSTM_basline.hdf5\n",
      "Epoch 56/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5650 - acc: 0.3817\n",
      "\n",
      "Epoch 00056: loss improved from 2.57952 to 2.56503, saving model to LSTM_basline.hdf5\n",
      "Epoch 57/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5676 - acc: 0.3813\n",
      "\n",
      "Epoch 00057: loss did not improve from 2.56503\n",
      "Epoch 58/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5442 - acc: 0.3880\n",
      "\n",
      "Epoch 00058: loss improved from 2.56503 to 2.54420, saving model to LSTM_basline.hdf5\n",
      "Epoch 59/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5174 - acc: 0.3933\n",
      "\n",
      "Epoch 00059: loss improved from 2.54420 to 2.51744, saving model to LSTM_basline.hdf5\n",
      "Epoch 60/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5558 - acc: 0.3854\n",
      "\n",
      "Epoch 00060: loss did not improve from 2.51744\n",
      "Epoch 61/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.5266 - acc: 0.3906\n",
      "\n",
      "Epoch 00061: loss did not improve from 2.51744\n",
      "Epoch 62/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4966 - acc: 0.3977\n",
      "\n",
      "Epoch 00062: loss improved from 2.51744 to 2.49659, saving model to LSTM_basline.hdf5\n",
      "Epoch 63/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4869 - acc: 0.4005\n",
      "\n",
      "Epoch 00063: loss improved from 2.49659 to 2.48692, saving model to LSTM_basline.hdf5\n",
      "Epoch 64/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4868 - acc: 0.3997\n",
      "\n",
      "Epoch 00064: loss improved from 2.48692 to 2.48677, saving model to LSTM_basline.hdf5\n",
      "Epoch 65/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4711 - acc: 0.4044\n",
      "\n",
      "Epoch 00065: loss improved from 2.48677 to 2.47112, saving model to LSTM_basline.hdf5\n",
      "Epoch 66/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4646 - acc: 0.4013\n",
      "\n",
      "Epoch 00066: loss improved from 2.47112 to 2.46458, saving model to LSTM_basline.hdf5\n",
      "Epoch 67/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4460 - acc: 0.4045\n",
      "\n",
      "Epoch 00067: loss improved from 2.46458 to 2.44605, saving model to LSTM_basline.hdf5\n",
      "Epoch 68/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4450 - acc: 0.4034\n",
      "\n",
      "Epoch 00068: loss improved from 2.44605 to 2.44501, saving model to LSTM_basline.hdf5\n",
      "Epoch 69/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.4400 - acc: 0.4107\n",
      "\n",
      "Epoch 00069: loss improved from 2.44501 to 2.44000, saving model to LSTM_basline.hdf5\n",
      "Epoch 70/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.4092 - acc: 0.4167\n",
      "\n",
      "Epoch 00070: loss improved from 2.44000 to 2.40922, saving model to LSTM_basline.hdf5\n",
      "Epoch 71/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.4075 - acc: 0.4150\n",
      "\n",
      "Epoch 00071: loss improved from 2.40922 to 2.40751, saving model to LSTM_basline.hdf5\n",
      "Epoch 72/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3879 - acc: 0.4204\n",
      "\n",
      "Epoch 00072: loss improved from 2.40751 to 2.38792, saving model to LSTM_basline.hdf5\n",
      "Epoch 73/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3870 - acc: 0.4188\n",
      "\n",
      "Epoch 00073: loss improved from 2.38792 to 2.38703, saving model to LSTM_basline.hdf5\n",
      "Epoch 74/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3878 - acc: 0.4231\n",
      "\n",
      "Epoch 00074: loss did not improve from 2.38703\n",
      "Epoch 75/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3815 - acc: 0.4194\n",
      "\n",
      "Epoch 00075: loss improved from 2.38703 to 2.38151, saving model to LSTM_basline.hdf5\n",
      "Epoch 76/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3530 - acc: 0.4279\n",
      "\n",
      "Epoch 00076: loss improved from 2.38151 to 2.35302, saving model to LSTM_basline.hdf5\n",
      "Epoch 77/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3778 - acc: 0.4217\n",
      "\n",
      "Epoch 00077: loss did not improve from 2.35302\n",
      "Epoch 78/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3526 - acc: 0.4251\n",
      "\n",
      "Epoch 00078: loss improved from 2.35302 to 2.35257, saving model to LSTM_basline.hdf5\n",
      "Epoch 79/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3572 - acc: 0.4232\n",
      "\n",
      "Epoch 00079: loss did not improve from 2.35257\n",
      "Epoch 80/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3464 - acc: 0.4248\n",
      "\n",
      "Epoch 00080: loss improved from 2.35257 to 2.34636, saving model to LSTM_basline.hdf5\n",
      "Epoch 81/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.3269 - acc: 0.4312\n",
      "\n",
      "Epoch 00081: loss improved from 2.34636 to 2.32691, saving model to LSTM_basline.hdf5\n",
      "Epoch 82/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3242 - acc: 0.4295\n",
      "\n",
      "Epoch 00082: loss improved from 2.32691 to 2.32416, saving model to LSTM_basline.hdf5\n",
      "Epoch 83/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3123 - acc: 0.4336\n",
      "\n",
      "Epoch 00083: loss improved from 2.32416 to 2.31234, saving model to LSTM_basline.hdf5\n",
      "Epoch 84/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2883 - acc: 0.4411\n",
      "\n",
      "Epoch 00084: loss improved from 2.31234 to 2.28833, saving model to LSTM_basline.hdf5\n",
      "Epoch 85/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2763 - acc: 0.4446\n",
      "\n",
      "Epoch 00085: loss improved from 2.28833 to 2.27627, saving model to LSTM_basline.hdf5\n",
      "Epoch 86/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2747 - acc: 0.4417\n",
      "\n",
      "Epoch 00086: loss improved from 2.27627 to 2.27467, saving model to LSTM_basline.hdf5\n",
      "Epoch 87/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2587 - acc: 0.4414\n",
      "\n",
      "Epoch 00087: loss improved from 2.27467 to 2.25874, saving model to LSTM_basline.hdf5\n",
      "Epoch 88/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2441 - acc: 0.4498\n",
      "\n",
      "Epoch 00088: loss improved from 2.25874 to 2.24408, saving model to LSTM_basline.hdf5\n",
      "Epoch 89/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2250 - acc: 0.4531\n",
      "\n",
      "Epoch 00089: loss improved from 2.24408 to 2.22497, saving model to LSTM_basline.hdf5\n",
      "Epoch 90/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2300 - acc: 0.4531\n",
      "\n",
      "Epoch 00090: loss did not improve from 2.22497\n",
      "Epoch 91/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.3297 - acc: 0.4225\n",
      "\n",
      "Epoch 00091: loss did not improve from 2.22497\n",
      "Epoch 92/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.2264 - acc: 0.4506\n",
      "\n",
      "Epoch 00092: loss did not improve from 2.22497\n",
      "Epoch 93/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.2306 - acc: 0.4513\n",
      "\n",
      "Epoch 00093: loss did not improve from 2.22497\n",
      "Epoch 94/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1885 - acc: 0.4607\n",
      "\n",
      "Epoch 00094: loss improved from 2.22497 to 2.18847, saving model to LSTM_basline.hdf5\n",
      "Epoch 95/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1967 - acc: 0.4597\n",
      "\n",
      "Epoch 00095: loss did not improve from 2.18847\n",
      "Epoch 96/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1658 - acc: 0.4667\n",
      "\n",
      "Epoch 00096: loss improved from 2.18847 to 2.16585, saving model to LSTM_basline.hdf5\n",
      "Epoch 97/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1459 - acc: 0.4714\n",
      "\n",
      "Epoch 00097: loss improved from 2.16585 to 2.14593, saving model to LSTM_basline.hdf5\n",
      "Epoch 98/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1837 - acc: 0.4608\n",
      "\n",
      "Epoch 00098: loss did not improve from 2.14593\n",
      "Epoch 99/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1540 - acc: 0.4678\n",
      "\n",
      "Epoch 00099: loss did not improve from 2.14593\n",
      "Epoch 100/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1255 - acc: 0.4766\n",
      "\n",
      "Epoch 00100: loss improved from 2.14593 to 2.12553, saving model to LSTM_basline.hdf5\n",
      "Epoch 101/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1343 - acc: 0.4723\n",
      "\n",
      "Epoch 00101: loss did not improve from 2.12553\n",
      "Epoch 102/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.1381 - acc: 0.4698\n",
      "\n",
      "Epoch 00102: loss did not improve from 2.12553\n",
      "Epoch 103/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0994 - acc: 0.4803\n",
      "\n",
      "Epoch 00103: loss improved from 2.12553 to 2.09937, saving model to LSTM_basline.hdf5\n",
      "Epoch 104/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0792 - acc: 0.4850\n",
      "\n",
      "Epoch 00104: loss improved from 2.09937 to 2.07917, saving model to LSTM_basline.hdf5\n",
      "Epoch 105/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.1004 - acc: 0.4788\n",
      "\n",
      "Epoch 00105: loss did not improve from 2.07917\n",
      "Epoch 106/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0798 - acc: 0.4858\n",
      "\n",
      "Epoch 00106: loss did not improve from 2.07917\n",
      "Epoch 107/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0601 - acc: 0.4897\n",
      "\n",
      "Epoch 00107: loss improved from 2.07917 to 2.06006, saving model to LSTM_basline.hdf5\n",
      "Epoch 108/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0441 - acc: 0.4942\n",
      "\n",
      "Epoch 00108: loss improved from 2.06006 to 2.04408, saving model to LSTM_basline.hdf5\n",
      "Epoch 109/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0517 - acc: 0.4896\n",
      "\n",
      "Epoch 00109: loss did not improve from 2.04408\n",
      "Epoch 110/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0273 - acc: 0.4958\n",
      "\n",
      "Epoch 00110: loss improved from 2.04408 to 2.02726, saving model to LSTM_basline.hdf5\n",
      "Epoch 111/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0203 - acc: 0.4987\n",
      "\n",
      "Epoch 00111: loss improved from 2.02726 to 2.02030, saving model to LSTM_basline.hdf5\n",
      "Epoch 112/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0270 - acc: 0.4958\n",
      "\n",
      "Epoch 00112: loss did not improve from 2.02030\n",
      "Epoch 113/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0108 - acc: 0.4988\n",
      "\n",
      "Epoch 00113: loss improved from 2.02030 to 2.01079, saving model to LSTM_basline.hdf5\n",
      "Epoch 114/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9923 - acc: 0.5025\n",
      "\n",
      "Epoch 00114: loss improved from 2.01079 to 1.99226, saving model to LSTM_basline.hdf5\n",
      "Epoch 115/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.0282 - acc: 0.4933\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.99226\n",
      "Epoch 116/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9773 - acc: 0.5087\n",
      "\n",
      "Epoch 00116: loss improved from 1.99226 to 1.97730, saving model to LSTM_basline.hdf5\n",
      "Epoch 117/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 2.0154 - acc: 0.4938\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.97730\n",
      "Epoch 118/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0800 - acc: 0.4754\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.97730\n",
      "Epoch 119/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 2.0045 - acc: 0.4961\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.97730\n",
      "Epoch 120/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.9959 - acc: 0.4965\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.97730\n",
      "Epoch 121/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9741 - acc: 0.5079\n",
      "\n",
      "Epoch 00121: loss improved from 1.97730 to 1.97413, saving model to LSTM_basline.hdf5\n",
      "Epoch 122/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9678 - acc: 0.5027\n",
      "\n",
      "Epoch 00122: loss improved from 1.97413 to 1.96779, saving model to LSTM_basline.hdf5\n",
      "Epoch 123/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9524 - acc: 0.5128\n",
      "\n",
      "Epoch 00123: loss improved from 1.96779 to 1.95240, saving model to LSTM_basline.hdf5\n",
      "Epoch 124/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.9160 - acc: 0.5223\n",
      "\n",
      "Epoch 00124: loss improved from 1.95240 to 1.91602, saving model to LSTM_basline.hdf5\n",
      "Epoch 125/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8929 - acc: 0.5259\n",
      "\n",
      "Epoch 00125: loss improved from 1.91602 to 1.89287, saving model to LSTM_basline.hdf5\n",
      "Epoch 126/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8939 - acc: 0.5235\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.89287\n",
      "Epoch 127/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8781 - acc: 0.5323\n",
      "\n",
      "Epoch 00127: loss improved from 1.89287 to 1.87809, saving model to LSTM_basline.hdf5\n",
      "Epoch 128/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8575 - acc: 0.5340\n",
      "\n",
      "Epoch 00128: loss improved from 1.87809 to 1.85747, saving model to LSTM_basline.hdf5\n",
      "Epoch 129/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8497 - acc: 0.5377\n",
      "\n",
      "Epoch 00129: loss improved from 1.85747 to 1.84974, saving model to LSTM_basline.hdf5\n",
      "Epoch 130/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8816 - acc: 0.5242\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.84974\n",
      "Epoch 131/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8366 - acc: 0.5385\n",
      "\n",
      "Epoch 00131: loss improved from 1.84974 to 1.83664, saving model to LSTM_basline.hdf5\n",
      "Epoch 132/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8491 - acc: 0.5344\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.83664\n",
      "Epoch 133/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8401 - acc: 0.5378\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.83664\n",
      "Epoch 134/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8288 - acc: 0.5417\n",
      "\n",
      "Epoch 00134: loss improved from 1.83664 to 1.82882, saving model to LSTM_basline.hdf5\n",
      "Epoch 135/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8244 - acc: 0.5392\n",
      "\n",
      "Epoch 00135: loss improved from 1.82882 to 1.82442, saving model to LSTM_basline.hdf5\n",
      "Epoch 136/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.8037 - acc: 0.5513\n",
      "\n",
      "Epoch 00136: loss improved from 1.82442 to 1.80373, saving model to LSTM_basline.hdf5\n",
      "Epoch 137/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7796 - acc: 0.5575\n",
      "\n",
      "Epoch 00137: loss improved from 1.80373 to 1.77963, saving model to LSTM_basline.hdf5\n",
      "Epoch 138/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7581 - acc: 0.5617\n",
      "\n",
      "Epoch 00138: loss improved from 1.77963 to 1.75811, saving model to LSTM_basline.hdf5\n",
      "Epoch 139/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7585 - acc: 0.5588\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.75811\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.8255 - acc: 0.5381\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.75811\n",
      "Epoch 141/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.7580 - acc: 0.5584\n",
      "\n",
      "Epoch 00141: loss improved from 1.75811 to 1.75800, saving model to LSTM_basline.hdf5\n",
      "Epoch 142/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7543 - acc: 0.5617\n",
      "\n",
      "Epoch 00142: loss improved from 1.75800 to 1.75430, saving model to LSTM_basline.hdf5\n",
      "Epoch 143/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7451 - acc: 0.5626\n",
      "\n",
      "Epoch 00143: loss improved from 1.75430 to 1.74511, saving model to LSTM_basline.hdf5\n",
      "Epoch 144/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7682 - acc: 0.5536\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.74511\n",
      "Epoch 145/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7500 - acc: 0.5581\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.74511\n",
      "Epoch 146/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7661 - acc: 0.5516\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.74511\n",
      "Epoch 147/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.8425 - acc: 0.5294\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.74511\n",
      "Epoch 148/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.7604 - acc: 0.5546\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.74511\n",
      "Epoch 149/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7190 - acc: 0.5646\n",
      "\n",
      "Epoch 00149: loss improved from 1.74511 to 1.71899, saving model to LSTM_basline.hdf5\n",
      "Epoch 150/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6664 - acc: 0.5817\n",
      "\n",
      "Epoch 00150: loss improved from 1.71899 to 1.66637, saving model to LSTM_basline.hdf5\n",
      "Epoch 151/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6641 - acc: 0.5829\n",
      "\n",
      "Epoch 00151: loss improved from 1.66637 to 1.66413, saving model to LSTM_basline.hdf5\n",
      "Epoch 152/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6688 - acc: 0.5806\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.66413\n",
      "Epoch 153/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6488 - acc: 0.5861\n",
      "\n",
      "Epoch 00153: loss improved from 1.66413 to 1.64877, saving model to LSTM_basline.hdf5\n",
      "Epoch 154/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6486 - acc: 0.5871\n",
      "\n",
      "Epoch 00154: loss improved from 1.64877 to 1.64858, saving model to LSTM_basline.hdf5\n",
      "Epoch 155/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6537 - acc: 0.5821\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.64858\n",
      "Epoch 156/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6134 - acc: 0.5991\n",
      "\n",
      "Epoch 00156: loss improved from 1.64858 to 1.61344, saving model to LSTM_basline.hdf5\n",
      "Epoch 157/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6091 - acc: 0.5986\n",
      "\n",
      "Epoch 00157: loss improved from 1.61344 to 1.60914, saving model to LSTM_basline.hdf5\n",
      "Epoch 158/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6423 - acc: 0.5836\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.60914\n",
      "Epoch 159/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6151 - acc: 0.5930\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.60914\n",
      "Epoch 160/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.6028 - acc: 0.5987\n",
      "\n",
      "Epoch 00160: loss improved from 1.60914 to 1.60277, saving model to LSTM_basline.hdf5\n",
      "Epoch 161/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5789 - acc: 0.6025\n",
      "\n",
      "Epoch 00161: loss improved from 1.60277 to 1.57888, saving model to LSTM_basline.hdf5\n",
      "Epoch 162/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5560 - acc: 0.6107\n",
      "\n",
      "Epoch 00162: loss improved from 1.57888 to 1.55596, saving model to LSTM_basline.hdf5\n",
      "Epoch 163/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5731 - acc: 0.6002\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.55596\n",
      "Epoch 164/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5642 - acc: 0.6043\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.55596\n",
      "Epoch 165/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5755 - acc: 0.6039\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.55596\n",
      "Epoch 166/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 2.3681 - acc: 0.4102\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.55596\n",
      "Epoch 167/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.9354 - acc: 0.4973\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.55596\n",
      "Epoch 168/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.7945 - acc: 0.5327\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.55596\n",
      "Epoch 169/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6837 - acc: 0.5678\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.55596\n",
      "Epoch 170/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.6470 - acc: 0.5806\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.55596\n",
      "Epoch 171/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5955 - acc: 0.5946\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.55596\n",
      "Epoch 172/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5548 - acc: 0.6043\n",
      "\n",
      "Epoch 00172: loss improved from 1.55596 to 1.55482, saving model to LSTM_basline.hdf5\n",
      "Epoch 173/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5407 - acc: 0.6099\n",
      "\n",
      "Epoch 00173: loss improved from 1.55482 to 1.54067, saving model to LSTM_basline.hdf5\n",
      "Epoch 174/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.5272 - acc: 0.6177\n",
      "\n",
      "Epoch 00174: loss improved from 1.54067 to 1.52719, saving model to LSTM_basline.hdf5\n",
      "Epoch 175/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.5106 - acc: 0.6207\n",
      "\n",
      "Epoch 00175: loss improved from 1.52719 to 1.51061, saving model to LSTM_basline.hdf5\n",
      "Epoch 176/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4989 - acc: 0.6217\n",
      "\n",
      "Epoch 00176: loss improved from 1.51061 to 1.49887, saving model to LSTM_basline.hdf5\n",
      "Epoch 177/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4617 - acc: 0.6356\n",
      "\n",
      "Epoch 00177: loss improved from 1.49887 to 1.46171, saving model to LSTM_basline.hdf5\n",
      "Epoch 178/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4506 - acc: 0.6337\n",
      "\n",
      "Epoch 00178: loss improved from 1.46171 to 1.45061, saving model to LSTM_basline.hdf5\n",
      "Epoch 179/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4601 - acc: 0.6330\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.45061\n",
      "Epoch 180/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4749 - acc: 0.6300\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.45061\n",
      "Epoch 181/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4445 - acc: 0.6330\n",
      "\n",
      "Epoch 00181: loss improved from 1.45061 to 1.44447, saving model to LSTM_basline.hdf5\n",
      "Epoch 182/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4579 - acc: 0.6354\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.44447\n",
      "Epoch 183/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4270 - acc: 0.6413\n",
      "\n",
      "Epoch 00183: loss improved from 1.44447 to 1.42702, saving model to LSTM_basline.hdf5\n",
      "Epoch 184/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.4182 - acc: 0.6431\n",
      "\n",
      "Epoch 00184: loss improved from 1.42702 to 1.41817, saving model to LSTM_basline.hdf5\n",
      "Epoch 185/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4020 - acc: 0.6485\n",
      "\n",
      "Epoch 00185: loss improved from 1.41817 to 1.40204, saving model to LSTM_basline.hdf5\n",
      "Epoch 186/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4477 - acc: 0.6321\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.40204\n",
      "Epoch 187/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4043 - acc: 0.6480\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.40204\n",
      "Epoch 188/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3776 - acc: 0.6558\n",
      "\n",
      "Epoch 00188: loss improved from 1.40204 to 1.37761, saving model to LSTM_basline.hdf5\n",
      "Epoch 189/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3983 - acc: 0.6526\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.37761\n",
      "Epoch 190/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.4129 - acc: 0.6432\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.37761\n",
      "Epoch 191/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3716 - acc: 0.6526\n",
      "\n",
      "Epoch 00191: loss improved from 1.37761 to 1.37159, saving model to LSTM_basline.hdf5\n",
      "Epoch 192/500\n",
      "12004/12004 [==============================] - 92s 8ms/step - loss: 1.3663 - acc: 0.6541\n",
      "\n",
      "Epoch 00192: loss improved from 1.37159 to 1.36629, saving model to LSTM_basline.hdf5\n",
      "Epoch 193/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.3432 - acc: 0.6625\n",
      "\n",
      "Epoch 00193: loss improved from 1.36629 to 1.34323, saving model to LSTM_basline.hdf5\n",
      "Epoch 194/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3512 - acc: 0.6555\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.34323\n",
      "Epoch 195/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3333 - acc: 0.6689\n",
      "\n",
      "Epoch 00195: loss improved from 1.34323 to 1.33325, saving model to LSTM_basline.hdf5\n",
      "Epoch 196/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3366 - acc: 0.6636\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.33325\n",
      "Epoch 197/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3284 - acc: 0.6689\n",
      "\n",
      "Epoch 00197: loss improved from 1.33325 to 1.32840, saving model to LSTM_basline.hdf5\n",
      "Epoch 198/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3174 - acc: 0.6709\n",
      "\n",
      "Epoch 00198: loss improved from 1.32840 to 1.31741, saving model to LSTM_basline.hdf5\n",
      "Epoch 199/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3216 - acc: 0.6679\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.31741\n",
      "Epoch 200/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 1.2829 - acc: 0.6782\n",
      "\n",
      "Epoch 00200: loss improved from 1.31741 to 1.28293, saving model to LSTM_basline.hdf5\n",
      "Epoch 201/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3151 - acc: 0.6644\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.28293\n",
      "Epoch 202/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.3230 - acc: 0.6637\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.28293\n",
      "Epoch 203/500\n",
      "12004/12004 [==============================] - 472s 39ms/step - loss: 1.3068 - acc: 0.6703\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.28293\n",
      "Epoch 204/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2923 - acc: 0.6715\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.28293\n",
      "Epoch 205/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2642 - acc: 0.6860\n",
      "\n",
      "Epoch 00205: loss improved from 1.28293 to 1.26424, saving model to LSTM_basline.hdf5\n",
      "Epoch 206/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2733 - acc: 0.6797\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.26424\n",
      "Epoch 207/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2532 - acc: 0.6829\n",
      "\n",
      "Epoch 00207: loss improved from 1.26424 to 1.25320, saving model to LSTM_basline.hdf5\n",
      "Epoch 208/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2485 - acc: 0.6886\n",
      "\n",
      "Epoch 00208: loss improved from 1.25320 to 1.24854, saving model to LSTM_basline.hdf5\n",
      "Epoch 209/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2588 - acc: 0.6815\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.24854\n",
      "Epoch 210/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2064 - acc: 0.7006\n",
      "\n",
      "Epoch 00210: loss improved from 1.24854 to 1.20635, saving model to LSTM_basline.hdf5\n",
      "Epoch 211/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1955 - acc: 0.7038\n",
      "\n",
      "Epoch 00211: loss improved from 1.20635 to 1.19546, saving model to LSTM_basline.hdf5\n",
      "Epoch 212/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2106 - acc: 0.6955\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.19546\n",
      "Epoch 213/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2247 - acc: 0.6934\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.19546\n",
      "Epoch 214/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.1773 - acc: 0.7048\n",
      "\n",
      "Epoch 00214: loss improved from 1.19546 to 1.17727, saving model to LSTM_basline.hdf5\n",
      "Epoch 215/500\n",
      "12004/12004 [==============================] - 576s 48ms/step - loss: 1.1887 - acc: 0.6991\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.17727\n",
      "Epoch 216/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.1623 - acc: 0.7064\n",
      "\n",
      "Epoch 00216: loss improved from 1.17727 to 1.16230, saving model to LSTM_basline.hdf5\n",
      "Epoch 217/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.2409 - acc: 0.6808\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.16230\n",
      "Epoch 218/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1935 - acc: 0.7007\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.16230\n",
      "Epoch 219/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1541 - acc: 0.7151\n",
      "\n",
      "Epoch 00219: loss improved from 1.16230 to 1.15409, saving model to LSTM_basline.hdf5\n",
      "Epoch 220/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1549 - acc: 0.7113\n",
      "\n",
      "Epoch 00220: loss did not improve from 1.15409\n",
      "Epoch 221/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1689 - acc: 0.7076\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.15409\n",
      "Epoch 222/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1235 - acc: 0.7222\n",
      "\n",
      "Epoch 00222: loss improved from 1.15409 to 1.12350, saving model to LSTM_basline.hdf5\n",
      "Epoch 223/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1501 - acc: 0.7129\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.12350\n",
      "Epoch 224/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1314 - acc: 0.7187\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.12350\n",
      "Epoch 225/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1155 - acc: 0.7222\n",
      "\n",
      "Epoch 00225: loss improved from 1.12350 to 1.11545, saving model to LSTM_basline.hdf5\n",
      "Epoch 226/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0936 - acc: 0.7291\n",
      "\n",
      "Epoch 00226: loss improved from 1.11545 to 1.09365, saving model to LSTM_basline.hdf5\n",
      "Epoch 227/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0802 - acc: 0.7310\n",
      "\n",
      "Epoch 00227: loss improved from 1.09365 to 1.08017, saving model to LSTM_basline.hdf5\n",
      "Epoch 228/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0787 - acc: 0.7323\n",
      "\n",
      "Epoch 00228: loss improved from 1.08017 to 1.07873, saving model to LSTM_basline.hdf5\n",
      "Epoch 229/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.2759 - acc: 0.6738\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.07873\n",
      "Epoch 230/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1748 - acc: 0.7002\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.07873\n",
      "Epoch 231/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.1318 - acc: 0.7154\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.07873\n",
      "Epoch 232/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0900 - acc: 0.7250\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.07873\n",
      "Epoch 233/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0472 - acc: 0.7395\n",
      "\n",
      "Epoch 00233: loss improved from 1.07873 to 1.04716, saving model to LSTM_basline.hdf5\n",
      "Epoch 234/500\n",
      "12004/12004 [==============================] - 901s 75ms/step - loss: 1.0285 - acc: 0.7453\n",
      "\n",
      "Epoch 00234: loss improved from 1.04716 to 1.02845, saving model to LSTM_basline.hdf5\n",
      "Epoch 235/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 1.0008 - acc: 0.7571\n",
      "\n",
      "Epoch 00235: loss improved from 1.02845 to 1.00083, saving model to LSTM_basline.hdf5\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0090 - acc: 0.7520\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.00083\n",
      "Epoch 237/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9938 - acc: 0.7585\n",
      "\n",
      "Epoch 00237: loss improved from 1.00083 to 0.99383, saving model to LSTM_basline.hdf5\n",
      "Epoch 238/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0087 - acc: 0.7501\n",
      "\n",
      "Epoch 00238: loss did not improve from 0.99383\n",
      "Epoch 239/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9798 - acc: 0.7617\n",
      "\n",
      "Epoch 00239: loss improved from 0.99383 to 0.97977, saving model to LSTM_basline.hdf5\n",
      "Epoch 240/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0141 - acc: 0.7485\n",
      "\n",
      "Epoch 00240: loss did not improve from 0.97977\n",
      "Epoch 241/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9675 - acc: 0.7655\n",
      "\n",
      "Epoch 00241: loss improved from 0.97977 to 0.96751, saving model to LSTM_basline.hdf5\n",
      "Epoch 242/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9836 - acc: 0.7580\n",
      "\n",
      "Epoch 00242: loss did not improve from 0.96751\n",
      "Epoch 243/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0117 - acc: 0.7486\n",
      "\n",
      "Epoch 00243: loss did not improve from 0.96751\n",
      "Epoch 244/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0034 - acc: 0.7480\n",
      "\n",
      "Epoch 00244: loss did not improve from 0.96751\n",
      "Epoch 245/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9560 - acc: 0.7666\n",
      "\n",
      "Epoch 00245: loss improved from 0.96751 to 0.95598, saving model to LSTM_basline.hdf5\n",
      "Epoch 246/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9641 - acc: 0.7636\n",
      "\n",
      "Epoch 00246: loss did not improve from 0.95598\n",
      "Epoch 247/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9518 - acc: 0.7667\n",
      "\n",
      "Epoch 00247: loss improved from 0.95598 to 0.95181, saving model to LSTM_basline.hdf5\n",
      "Epoch 248/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9533 - acc: 0.7611\n",
      "\n",
      "Epoch 00248: loss did not improve from 0.95181\n",
      "Epoch 249/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9940 - acc: 0.7506\n",
      "\n",
      "Epoch 00249: loss did not improve from 0.95181\n",
      "Epoch 250/500\n",
      "12004/12004 [==============================] - 1142s 95ms/step - loss: 0.9561 - acc: 0.7571\n",
      "\n",
      "Epoch 00250: loss did not improve from 0.95181\n",
      "Epoch 251/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9144 - acc: 0.7766\n",
      "\n",
      "Epoch 00251: loss improved from 0.95181 to 0.91444, saving model to LSTM_basline.hdf5\n",
      "Epoch 252/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9428 - acc: 0.7623\n",
      "\n",
      "Epoch 00252: loss did not improve from 0.91444\n",
      "Epoch 253/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.9941 - acc: 0.7477\n",
      "\n",
      "Epoch 00253: loss did not improve from 0.91444\n",
      "Epoch 254/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 1.0055 - acc: 0.7421\n",
      "\n",
      "Epoch 00254: loss did not improve from 0.91444\n",
      "Epoch 255/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9306 - acc: 0.7688\n",
      "\n",
      "Epoch 00255: loss did not improve from 0.91444\n",
      "Epoch 256/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.9113 - acc: 0.7737\n",
      "\n",
      "Epoch 00256: loss improved from 0.91444 to 0.91131, saving model to LSTM_basline.hdf5\n",
      "Epoch 257/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8963 - acc: 0.7787\n",
      "\n",
      "Epoch 00257: loss improved from 0.91131 to 0.89632, saving model to LSTM_basline.hdf5\n",
      "Epoch 258/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8598 - acc: 0.7894\n",
      "\n",
      "Epoch 00258: loss improved from 0.89632 to 0.85981, saving model to LSTM_basline.hdf5\n",
      "Epoch 259/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8556 - acc: 0.7947\n",
      "\n",
      "Epoch 00259: loss improved from 0.85981 to 0.85555, saving model to LSTM_basline.hdf5\n",
      "Epoch 260/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8867 - acc: 0.7779\n",
      "\n",
      "Epoch 00260: loss did not improve from 0.85555\n",
      "Epoch 261/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8540 - acc: 0.7904\n",
      "\n",
      "Epoch 00261: loss improved from 0.85555 to 0.85399, saving model to LSTM_basline.hdf5\n",
      "Epoch 262/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8491 - acc: 0.7932\n",
      "\n",
      "Epoch 00262: loss improved from 0.85399 to 0.84913, saving model to LSTM_basline.hdf5\n",
      "Epoch 263/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8364 - acc: 0.7957\n",
      "\n",
      "Epoch 00263: loss improved from 0.84913 to 0.83639, saving model to LSTM_basline.hdf5\n",
      "Epoch 264/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7953 - acc: 0.8094\n",
      "\n",
      "Epoch 00264: loss improved from 0.83639 to 0.79534, saving model to LSTM_basline.hdf5\n",
      "Epoch 265/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8458 - acc: 0.7921\n",
      "\n",
      "Epoch 00265: loss did not improve from 0.79534\n",
      "Epoch 266/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8920 - acc: 0.7772\n",
      "\n",
      "Epoch 00266: loss did not improve from 0.79534\n",
      "Epoch 267/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8378 - acc: 0.7951\n",
      "\n",
      "Epoch 00267: loss did not improve from 0.79534\n",
      "Epoch 268/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8317 - acc: 0.7946\n",
      "\n",
      "Epoch 00268: loss did not improve from 0.79534\n",
      "Epoch 269/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8377 - acc: 0.7910\n",
      "\n",
      "Epoch 00269: loss did not improve from 0.79534\n",
      "Epoch 270/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.8029 - acc: 0.8054\n",
      "\n",
      "Epoch 00270: loss did not improve from 0.79534\n",
      "Epoch 271/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7643 - acc: 0.8161\n",
      "\n",
      "Epoch 00271: loss improved from 0.79534 to 0.76425, saving model to LSTM_basline.hdf5\n",
      "Epoch 272/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7991 - acc: 0.8036\n",
      "\n",
      "Epoch 00272: loss did not improve from 0.76425\n",
      "Epoch 273/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7792 - acc: 0.8095\n",
      "\n",
      "Epoch 00273: loss did not improve from 0.76425\n",
      "Epoch 274/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.7835 - acc: 0.8077\n",
      "\n",
      "Epoch 00274: loss did not improve from 0.76425\n",
      "Epoch 275/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7820 - acc: 0.8091\n",
      "\n",
      "Epoch 00275: loss did not improve from 0.76425\n",
      "Epoch 276/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7921 - acc: 0.8064\n",
      "\n",
      "Epoch 00276: loss did not improve from 0.76425\n",
      "Epoch 277/500\n",
      "12004/12004 [==============================] - 13s 1ms/step - loss: 0.7498 - acc: 0.8212\n",
      "\n",
      "Epoch 00277: loss improved from 0.76425 to 0.74979, saving model to LSTM_basline.hdf5\n",
      "Epoch 278/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7488 - acc: 0.8174\n",
      "\n",
      "Epoch 00278: loss improved from 0.74979 to 0.74875, saving model to LSTM_basline.hdf5\n",
      "Epoch 279/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7066 - acc: 0.8356\n",
      "\n",
      "Epoch 00279: loss improved from 0.74875 to 0.70661, saving model to LSTM_basline.hdf5\n",
      "Epoch 280/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.7235 - acc: 0.8291\n",
      "\n",
      "Epoch 00280: loss did not improve from 0.70661\n",
      "Epoch 281/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7413 - acc: 0.8196\n",
      "\n",
      "Epoch 00281: loss did not improve from 0.70661\n",
      "Epoch 282/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6969 - acc: 0.8343\n",
      "\n",
      "Epoch 00282: loss improved from 0.70661 to 0.69694, saving model to LSTM_basline.hdf5\n",
      "Epoch 283/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7053 - acc: 0.8263\n",
      "\n",
      "Epoch 00283: loss did not improve from 0.69694\n",
      "Epoch 284/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7555 - acc: 0.8148\n",
      "\n",
      "Epoch 00284: loss did not improve from 0.69694\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7616 - acc: 0.8085\n",
      "\n",
      "Epoch 00285: loss did not improve from 0.69694\n",
      "Epoch 286/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7461 - acc: 0.8152\n",
      "\n",
      "Epoch 00286: loss did not improve from 0.69694\n",
      "Epoch 287/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7726 - acc: 0.8076\n",
      "\n",
      "Epoch 00287: loss did not improve from 0.69694\n",
      "Epoch 288/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7708 - acc: 0.8017\n",
      "\n",
      "Epoch 00288: loss did not improve from 0.69694\n",
      "Epoch 289/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7291 - acc: 0.8201\n",
      "\n",
      "Epoch 00289: loss did not improve from 0.69694\n",
      "Epoch 290/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7545 - acc: 0.8155\n",
      "\n",
      "Epoch 00290: loss did not improve from 0.69694\n",
      "Epoch 291/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7521 - acc: 0.8103\n",
      "\n",
      "Epoch 00291: loss did not improve from 0.69694\n",
      "Epoch 292/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.7082 - acc: 0.8263\n",
      "\n",
      "Epoch 00292: loss did not improve from 0.69694\n",
      "Epoch 293/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6982 - acc: 0.8336\n",
      "\n",
      "Epoch 00293: loss did not improve from 0.69694\n",
      "Epoch 294/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6420 - acc: 0.8513\n",
      "\n",
      "Epoch 00294: loss improved from 0.69694 to 0.64199, saving model to LSTM_basline.hdf5\n",
      "Epoch 295/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6465 - acc: 0.8489\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.64199\n",
      "Epoch 296/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6363 - acc: 0.8542\n",
      "\n",
      "Epoch 00296: loss improved from 0.64199 to 0.63629, saving model to LSTM_basline.hdf5\n",
      "Epoch 297/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6021 - acc: 0.8615\n",
      "\n",
      "Epoch 00297: loss improved from 0.63629 to 0.60208, saving model to LSTM_basline.hdf5\n",
      "Epoch 298/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5974 - acc: 0.8627\n",
      "\n",
      "Epoch 00298: loss improved from 0.60208 to 0.59736, saving model to LSTM_basline.hdf5\n",
      "Epoch 299/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5933 - acc: 0.8642\n",
      "\n",
      "Epoch 00299: loss improved from 0.59736 to 0.59333, saving model to LSTM_basline.hdf5\n",
      "Epoch 300/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5828 - acc: 0.8685\n",
      "\n",
      "Epoch 00300: loss improved from 0.59333 to 0.58277, saving model to LSTM_basline.hdf5\n",
      "Epoch 301/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5694 - acc: 0.8728\n",
      "\n",
      "Epoch 00301: loss improved from 0.58277 to 0.56935, saving model to LSTM_basline.hdf5\n",
      "Epoch 302/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5438 - acc: 0.8819\n",
      "\n",
      "Epoch 00302: loss improved from 0.56935 to 0.54378, saving model to LSTM_basline.hdf5\n",
      "Epoch 303/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6333 - acc: 0.8496\n",
      "\n",
      "Epoch 00303: loss did not improve from 0.54378\n",
      "Epoch 304/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5997 - acc: 0.8610\n",
      "\n",
      "Epoch 00304: loss did not improve from 0.54378\n",
      "Epoch 305/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5641 - acc: 0.8742\n",
      "\n",
      "Epoch 00305: loss did not improve from 0.54378\n",
      "Epoch 306/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5643 - acc: 0.8740\n",
      "\n",
      "Epoch 00306: loss did not improve from 0.54378\n",
      "Epoch 307/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6052 - acc: 0.8576\n",
      "\n",
      "Epoch 00307: loss did not improve from 0.54378\n",
      "Epoch 308/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5883 - acc: 0.8619\n",
      "\n",
      "Epoch 00308: loss did not improve from 0.54378\n",
      "Epoch 309/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5857 - acc: 0.8627\n",
      "\n",
      "Epoch 00309: loss did not improve from 0.54378\n",
      "Epoch 310/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5575 - acc: 0.8753\n",
      "\n",
      "Epoch 00310: loss did not improve from 0.54378\n",
      "Epoch 311/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5644 - acc: 0.8721\n",
      "\n",
      "Epoch 00311: loss did not improve from 0.54378\n",
      "Epoch 312/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5679 - acc: 0.8697\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.54378\n",
      "Epoch 313/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5709 - acc: 0.8642\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.54378\n",
      "Epoch 314/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5753 - acc: 0.8635\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.54378\n",
      "Epoch 315/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5390 - acc: 0.8758\n",
      "\n",
      "Epoch 00315: loss improved from 0.54378 to 0.53905, saving model to LSTM_basline.hdf5\n",
      "Epoch 316/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6008 - acc: 0.8540\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.53905\n",
      "Epoch 317/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.6118 - acc: 0.8444\n",
      "\n",
      "Epoch 00317: loss did not improve from 0.53905\n",
      "Epoch 318/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.5450 - acc: 0.8720\n",
      "\n",
      "Epoch 00318: loss did not improve from 0.53905\n",
      "Epoch 319/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4866 - acc: 0.8945\n",
      "\n",
      "Epoch 00319: loss improved from 0.53905 to 0.48657, saving model to LSTM_basline.hdf5\n",
      "Epoch 320/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4617 - acc: 0.9018\n",
      "\n",
      "Epoch 00320: loss improved from 0.48657 to 0.46167, saving model to LSTM_basline.hdf5\n",
      "Epoch 321/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4765 - acc: 0.8960\n",
      "\n",
      "Epoch 00321: loss did not improve from 0.46167\n",
      "Epoch 322/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4727 - acc: 0.8981\n",
      "\n",
      "Epoch 00322: loss did not improve from 0.46167\n",
      "Epoch 323/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4873 - acc: 0.8918\n",
      "\n",
      "Epoch 00323: loss did not improve from 0.46167\n",
      "Epoch 324/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4624 - acc: 0.9014\n",
      "\n",
      "Epoch 00324: loss did not improve from 0.46167\n",
      "Epoch 325/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4335 - acc: 0.9106\n",
      "\n",
      "Epoch 00325: loss improved from 0.46167 to 0.43351, saving model to LSTM_basline.hdf5\n",
      "Epoch 326/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4435 - acc: 0.9067\n",
      "\n",
      "Epoch 00326: loss did not improve from 0.43351\n",
      "Epoch 327/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4454 - acc: 0.9047\n",
      "\n",
      "Epoch 00327: loss did not improve from 0.43351\n",
      "Epoch 328/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4372 - acc: 0.9061\n",
      "\n",
      "Epoch 00328: loss did not improve from 0.43351\n",
      "Epoch 329/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4505 - acc: 0.9010\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.43351\n",
      "Epoch 330/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4957 - acc: 0.8838\n",
      "\n",
      "Epoch 00330: loss did not improve from 0.43351\n",
      "Epoch 331/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4646 - acc: 0.8930\n",
      "\n",
      "Epoch 00331: loss did not improve from 0.43351\n",
      "Epoch 332/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4296 - acc: 0.9078\n",
      "\n",
      "Epoch 00332: loss improved from 0.43351 to 0.42963, saving model to LSTM_basline.hdf5\n",
      "Epoch 333/500\n",
      "12004/12004 [==============================] - 14s 1ms/step - loss: 0.4911 - acc: 0.8829\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.42963\n",
      "Epoch 334/500\n",
      "12004/12004 [==============================] - 2053s 171ms/step - loss: 0.4064 - acc: 0.9140\n",
      "\n",
      "Epoch 00334: loss improved from 0.42963 to 0.40635, saving model to LSTM_basline.hdf5\n",
      "Epoch 335/500\n",
      "12004/12004 [==============================] - 15s 1ms/step - loss: 0.4972 - acc: 0.8815\n",
      "\n",
      "Epoch 00335: loss did not improve from 0.40635\n",
      "Epoch 336/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11800/12004 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.8738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-cdd54980823c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlps/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "result_history = model.fit(X, y, batch_size=200, epochs= 500, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "general to your ability to return that excess capital i heard your comments about redeploying if you not allowed but\n",
      "\n",
      "Generated text:\n",
      "it seems like your rwa reduction in housing of commercial is that the last couple of products people you getting a little bit to volatility to client mix of actually seems the worries been trying to irrational this in the ficc are lining color that you saw the reserve as the main activity should icg something the market went average go down and i heard all the opportunities conversation that i curious if retail relates how there any not tradeoffs and revenues maybe far of the same markets and you seems like you do have reserved and jamie to that\n"
     ]
    }
   ],
   "source": [
    "#test 1\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "# model = load_model('model.h6')\n",
    "\n",
    "model.load_weights(filepath)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text:\n",
      "much of that can fall to the bottom line because there a lot of optimism about what can happen if\n",
      "\n",
      "Generated text:\n",
      "stocks have moved well we expecting that to move to the bottom line the big concern that people have is that branded out a lot of people billion the portfolio ticking down a little bit in your comments so i heard all the whole gains in hit in your words\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "# load cleaned text sequences\n",
    "in_filename = 'Glenn_Schorr_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    " \n",
    "# load the model\n",
    "# model = load_model('model.h6')\n",
    "\n",
    "model.load_weights(filepath)\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = plk.load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('Initial text:')\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "print('Generated text:')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
