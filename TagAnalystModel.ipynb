{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, make_scorer, accuracy_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/qaData.csv', parse_dates=['Date'])\n",
    "data['EarningTag2'] = data['EarningTag2'].str.strip()\n",
    "\n",
    "#Add Lagged Column\n",
    "data['Lag1'] = data.groupby([\"Company\", \"Participants\", \"Date\", \"EventName\", \"EventType\"])['EarningTag2'].shift(1)\n",
    "\n",
    "#Add Year and Month from Data\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "\n",
    "#Drop non-earning calls\n",
    "nn_data = data.loc[data['EventType']==\"Earnings call\", ['Company', 'Participants', 'Month', 'Year', 'AnalystName',\t'AnalystCompany', 'EventName', 'Lag1', 'EarningTag2']].copy()\n",
    "#Add quarter\n",
    "nn_data['Quarter'] = nn_data['EventName'].str.split(\"Q\").str[0]\n",
    "#Drop bad features\n",
    "nn_data = nn_data[['Company', \"Participants\", \"AnalystName\", \"AnalystCompany\", \"Month\", \"Year\", \"Quarter\", \"Lag1\", \"EarningTag2\"]].copy()\n",
    "\n",
    "nn_data['NewIndex'] =   nn_data['AnalystName'].str.replace(\" \", \"\") +  \"_Y\" + \\\n",
    "                        nn_data['Year'].astype(str) + \"_M\" + nn_data['Month'].astype(str) + \"_Q\" + \\\n",
    "                        nn_data['Quarter'].astype(str)\n",
    "pct_data = (nn_data.groupby(['NewIndex', \"EarningTag2\"]).size().reset_index()).pivot(index='NewIndex', columns='EarningTag2', values=0).fillna(0)\n",
    "\n",
    "#pct_div_data = pct_data.div(pct_data.sum(axis=1), axis=0)\n",
    "pct_div_data = pct_data\n",
    "pct_div_data = pd.concat([pct_div_data.reset_index(drop=True), pct_div_data.reset_index()['NewIndex'].str.split(\"_\", expand=True)[[0, 1, 2, 3]]], axis=1, ignore_index=True)\n",
    "pct_div_data.columns = pct_data.columns.tolist() + ['AnalystName', 'Year', 'Month', 'Quarter']\n",
    "pct_div_data = pct_div_data[['AnalystName', 'Year', 'Month', 'Quarter'] + pct_data.columns.tolist()]\n",
    "\n",
    "pct_melt_data = pd.melt(pct_div_data, id_vars=['AnalystName', 'Year', 'Month', 'Quarter'], var_name='Tag', value_name='NumQ')\n",
    "pct_melt_data = pd.concat([pct_melt_data, \n",
    "                             pd.get_dummies(pct_melt_data['AnalystName'], prefix='A', prefix_sep=\"\"),\n",
    "                             pd.get_dummies(pct_melt_data['Month']),\n",
    "                             pd.get_dummies(pct_melt_data['Quarter']),\n",
    "                             pd.get_dummies(pct_melt_data['Year']),\n",
    "                             pd.get_dummies(pct_melt_data['Tag'], prefix='T', prefix_sep=\"\")], axis=1)\n",
    "pct_melt_data = pct_melt_data.drop(['AnalystName', 'Year', 'Month', 'Quarter', 'Tag'], axis=1)\n",
    "pct_melt_data = pct_melt_data.reset_index(drop=True)\n",
    "pct_melt_data['NumQ'] = pct_melt_data['NumQ'].astype(bool).astype(int)\n",
    "\n",
    "train, test = pct_melt_data.loc[pct_melt_data['Y2018']!=1].copy().reset_index(drop=True), \\\n",
    "                pct_melt_data.loc[pct_melt_data['Y2018']==1].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ'], axis=1), train['NumQ'].values\n",
    "X_test, y_test = test.drop(['NumQ'], axis=1), test['NumQ'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(50)\n",
    "scores_gbc = np.zeros(50)\n",
    "scores_rf = np.zeros(50)\n",
    "\n",
    "for comp in range(1, 50):\n",
    "    model = NMF(n_components=comp)\n",
    "    X_train_W = model.fit_transform(X_train)\n",
    "    X_test_W = model.transform(X_test)\n",
    "    \n",
    "    estimator = LogisticRegression().fit(X_train_W, y_train)\n",
    "    preds = estimator.predict_proba(X_test_W)[:,1]\n",
    "    scores[comp] = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    estimator_gbc = GradientBoostingClassifier(warm_start=True).fit(X_train_W, y_train)\n",
    "    preds_gbc = estimator_gbc.predict_proba(X_test_W)[:,1]\n",
    "    scores_gbc[comp] = roc_auc_score(y_test, preds_gbc)\n",
    "    \n",
    "    estimator_rf = RandomForestClassifier(warm_start=True).fit(X_train_W, y_train)\n",
    "    preds_rf = estimator_rf.predict_proba(X_test_W)[:,1]\n",
    "    scores_rf[comp] = roc_auc_score(y_test, preds_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611043685046985\n",
      "0.7789302809280225\n",
      "0.7566335118232973\n"
     ]
    }
   ],
   "source": [
    "print(scores[1:].max())\n",
    "print(scores_gbc[1:].max())\n",
    "print(scores_rf[1:].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_rf[1:].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18]), 'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NMF(n_components=42).fit(X_train)\n",
    "X_train_W = model.transform(X_train)\n",
    "\n",
    "param_grid = {'min_samples_split': np.arange(2, 20,2, dtype=int),\n",
    "              'max_depth': np.arange(1, 10, 1, dtype=int),\n",
    "              'min_samples_leaf': np.arange(1, 20, 1, dtype=int)}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(criterion='entropy'), cv=5, param_grid=param_grid, return_train_score=False, scoring=make_scorer(roc_auc_score))\n",
    "grid.fit(X_train_W, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'min_samples_leaf': 11, 'min_samples_split': 18}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683920174843479"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NMF(n_components=42).fit(X_train)\n",
    "X_train_W = model.transform(X_train)\n",
    "X_test_W = model.transform(X_test)\n",
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', max_depth=2, min_samples_leaf=11, min_samples_split=18).fit(X_train_W, y_train)\n",
    "roc_auc_score(y_test, estimator.predict_proba(X_test_W)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Success!\n"
     ]
    }
   ],
   "source": [
    "print(\"Great Success!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
