{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import textacy\n",
    "from textacy import preprocess_text, Doc, Corpus\n",
    "from textacy.vsm import Vectorizer, GroupVectorizer\n",
    "from textacy.tm import TopicModel\n",
    "en = textacy.load_spacy(\"en_core_web_sm\", disable='parser')\n",
    "\n",
    "data_directory = '/'.join(os.getcwd().split(\"/\")[:-1]) + '/data/'\n",
    "\n",
    "test_set = [173,  74,  20, 101,  83,   1,  38,  39,  72,  50,  21, 164,  57,\n",
    "       169,   8,  63, 102,  34,  80, 192, 139,  88, 112, 116,  61,  46,\n",
    "        51, 165, 135,  89, 108,   7,  25,  15, 125,  93, 130,  71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orig_data = pd.read_csv(data_directory + 'qaData.csv', parse_dates=['Date'])\n",
    "orig_data['Year'] = orig_data['Date'].dt.year\n",
    "orig_data['Month'] = orig_data['Date'].dt.month\n",
    "orig_data['Quarter'] = orig_data['Month'].apply(lambda x: 1 if x < 4 else 2 if x < 7 else 3 if x < 9 else 4)\n",
    "orig_data['Company'] = orig_data['Company'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['EventType'] = orig_data['EventType'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Participants'] = orig_data['Participants'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystName'] = orig_data['AnalystName'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['AnalystCompany'] = orig_data['AnalystCompany'].str.title().str.replace(\" \", \"\")\n",
    "orig_data['Tag'] = orig_data['EarningTag2'].str.title().str.replace(\" \", \"\")\n",
    "\n",
    "orig_data = orig_data.loc[~orig_data['AnalystName'].isna()].copy()\n",
    "\n",
    "groups = []\n",
    "for i, (name, group) in enumerate(orig_data.groupby(['Company', 'Participants', 'Month', 'Year', 'Quarter', 'EventType', 'Date'])):\n",
    "    g2 = group.copy()\n",
    "    g2['EventNumber'] = i\n",
    "    groups.append(g2)\n",
    "    \n",
    "indexed_data = pd.concat(groups)\n",
    "#train_data = indexed_data.loc[~indexed_data['EventNumber'].isin(test_set)].copy().reset_index(drop=True)\n",
    "\n",
    "q_data = indexed_data[['Date', 'EventNumber', 'Year', 'Quarter', 'Company', 'AnalystName', 'EventType', 'Tag', 'Question']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = Corpus(lang=en, docs=q_data.apply(lambda x: Doc(content=' '.join(\n",
    "                                                        [token for token in preprocess_text(text=x['Question'], lowercase=True, no_punct=True, no_contractions=True, no_accents=True, no_currency_symbols=True, no_numbers=True).split(' ') if len(token)>2]),\n",
    "                                                    lang=en, metadata={'Year':x['Year'],\n",
    "                                                                       'Quarter':x['Quarter'],\n",
    "                                                                       'Company':x['Company'],\n",
    "                                                                       'Analyst':x[\"AnalystName\"],\n",
    "                                                                       'Tag':x['Tag'],\n",
    "                                                                       'EventType':x['EventType'],\n",
    "                                                                       'EventNumber':x['EventNumber']}),axis=1).tolist())\n",
    "tokenized_docs = [list(doc.to_terms_list(ngrams=(1), as_strings=True, normalize='lemma', drop_determiners=True)) for doc in docs]\n",
    "\n",
    "bigram_phraser = Phraser(Phrases(tokenized_docs, min_count=10, threshold=20, delimiter=b' '))\n",
    "bigram_docs = [bigram_phraser[doc] for doc in tokenized_docs] \n",
    "\n",
    "trigram_phraser = Phraser(Phrases(bigram_docs, min_count=5, threshold=10, delimiter=b' '))\n",
    "trigram_docs = [trigram_phraser[doc] for doc in bigram_docs]\n",
    "\n",
    "q_list = [{'Year':docs[i].metadata['Year'], \n",
    "            'Quarter':\"Q{}\".format(docs[i].metadata['Quarter']), \n",
    "            'Company':docs[i].metadata['Company'], \n",
    "            'Analyst':docs[i].metadata['Analyst'], \n",
    "            'Tag':docs[i].metadata['Tag'],\n",
    "            'EventType':docs[i].metadata['EventType'], \n",
    "            'EventNumber':docs[i].metadata['EventNumber'],\n",
    "            'Question':trigram_docs[i]} for i in range(len(trigram_docs))]\n",
    "\n",
    "#meta_docs = [doc + [\"{}\".format(q_list[i]['Year']), q_list[i]['Quarter'], q_list[i]['Company']] for i, doc in enumerate(trigram_docs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_topic_list = []\n",
    "\n",
    "analysts = q_data['AnalystName'].unique().tolist()\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "topic_cols = [\"t{}\".format(i) for i in range(NUM_TOPICS)]\n",
    "\n",
    "for i in q_data['EventNumber'].unique():\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    a_dict = {a:[] for a in analysts}\n",
    "\n",
    "    for doc in q_list:\n",
    "        if doc['EventNumber'] < i:\n",
    "            a_dict[doc['Analyst']] += doc['Question'] \n",
    "        \n",
    "    a_docs = [v for k,v in a_dict.items()]\n",
    "    a_list = [{'Analyst':k, 'Words':v} for k,v in a_dict.items()]\n",
    "\n",
    "    vec = Vectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(a_docs)\n",
    "    doc_term_matrix = vec.transform(a_docs)\n",
    "\n",
    "    model = TopicModel('nmf', n_topics=NUM_TOPICS)\n",
    "    model.fit(doc_term_matrix)\n",
    "    doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "    a_topic_dict = {}\n",
    "\n",
    "    for doc, topic in model.top_doc_topics(doc_topic_matrix, docs=-1, top_n=-1, weights=True):\n",
    "        a_topic_dict[a_list[doc]['Analyst']] = {\"t{}\".format(k):v for k,v in topic}\n",
    "\n",
    "    a_t = pd.DataFrame.from_dict(a_topic_dict, orient='index').reset_index().fillna(0)\n",
    "    a_t = a_t[['index'] + topic_cols]\n",
    "    a_t.columns = ['AnalystName'] + topic_cols\n",
    "    a_t['EventNumber'] = i\n",
    "    a_topic_list.append(a_t)\n",
    "\n",
    "a_doc_term_matrix = doc_topic_matrix.copy()\n",
    "\n",
    "a_topic_df = pd.concat(a_topic_list)\n",
    "a_topic_df['tMax'] = a_topic_df[topic_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_topic_df.to_csv(data_directory+\"analystTopic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_topic_list = []\n",
    "\n",
    "tags = q_data['Tag'].unique().tolist()\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "topic_cols = [\"t{}\".format(i) for i in range(NUM_TOPICS)]\n",
    "\n",
    "for i in q_data['EventNumber'].unique():\n",
    "    if i == 0:\n",
    "        continue\n",
    "    \n",
    "    t_dict = {t:[] for t in tags}\n",
    "\n",
    "    for doc in q_list:\n",
    "        if doc['EventNumber'] < i:\n",
    "            t_dict[doc['Tag']] += doc['Question'] \n",
    "        \n",
    "    t_docs = [v for k,v in t_dict.items()]\n",
    "    t_list = [{'Tag':k, 'Words':v} for k,v in t_dict.items()]\n",
    "\n",
    "    vec = Vectorizer(tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear').fit(t_docs)\n",
    "    doc_term_matrix = vec.transform(t_docs)\n",
    "\n",
    "    model = TopicModel('nmf', n_topics=NUM_TOPICS)\n",
    "    model.fit(doc_term_matrix)\n",
    "    doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "    t_topic_dict = {}\n",
    "\n",
    "    for doc, topic in model.top_doc_topics(doc_topic_matrix, docs=-1, top_n=-1, weights=True):\n",
    "        t_topic_dict[t_list[doc]['Tag']] = {\"t{}\".format(k):v for k,v in topic}\n",
    "\n",
    "    t_t = pd.DataFrame.from_dict(t_topic_dict, orient='index').reset_index().fillna(0)\n",
    "    t_t = t_t[['index'] + topic_cols]\n",
    "    t_t.columns = ['Tag'] + topic_cols\n",
    "    t_t['EventNumber'] = i\n",
    "    t_topic_list.append(t_t)\n",
    "    \n",
    "t_doc_term_matrix = doc_topic_matrix.copy()\n",
    "\n",
    "t_topic_df = pd.concat(t_topic_list)\n",
    "t_topic_df['tMax'] = t_topic_df[topic_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_doc_term_matrix.T(t_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.5740467 ,  6.39871547, 10.86417935, ..., 17.25449029,\n",
       "         6.51700495,  3.96572495],\n",
       "       [ 2.76958947,  0.        ,  0.        , ...,  4.29012093,\n",
       "         0.        ,  2.14244999],\n",
       "       [ 4.65533809,  3.84143165,  4.30523789, ..., 10.02205078,\n",
       "         2.69667457,  3.19657974],\n",
       "       ...,\n",
       "       [ 0.44639927,  0.        ,  0.92406501, ...,  0.9784097 ,\n",
       "         0.52581223,  0.27669521],\n",
       "       [ 2.75606472,  0.84471951,  5.6238114 , ...,  7.47807263,\n",
       "         3.13157921,  1.51703465],\n",
       "       [ 0.2960852 ,  0.12903673,  0.47813936, ...,  0.69597067,\n",
       "         0.29836503,  0.19353779]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_doc_term_matrix.dot(t_doc_term_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : weak   fourth quarter   see   think   number   rate   market   fee   maybe   quarter\n",
      "\t-Cib: 7.709\n",
      "\t-Revenue: 5.738\n",
      "\t-Awm: 5.717\n",
      "\t-Expenses: 5.7\n",
      "topic 1 : scb   basel   propose   dividend payout ratio   stress test   minimis   capital ratio   stress capital buffer   gsib buffer   tier number\n",
      "\t-Capital: 11.43\n",
      "\t-RegulatoryTopics: 10.27\n",
      "\t-BalanceSheet: 1.637\n",
      "\t-AccountingAndTaxes: 0.9872\n",
      "topic 2 : reserve release   redetermination   share national   credit exam   oil price   non accrual   non investment grade   unfunded   charge off   oil gas\n",
      "\t-CreditCosts: 13.08\n",
      "\t-MacroeconomicUpdate: 1.886\n",
      "\t-BalanceSheet: 1.021\n",
      "\t-Cb: 1.019\n",
      "topic 3 : promotional   new account   mortgage banking   promotional balance   zelle   origination   promotion   airline   credit card   retail partner\n",
      "\t-Ccb: 14.49\n",
      "\t-BalanceSheet: 2.563\n",
      "\t-Awm: 0.7186\n",
      "\t-Cb: 0.4639\n",
      "topic 4 : blockchain   buy stock   stakeholder   investment spending   selling   proxy   tangible book   digitalization   stake   culture\n",
      "\t-OtherTopics: 14.47\n",
      "\t-Expenses: 2.38\n",
      "\t-Capital: 0.3299\n",
      "\t-AccountingAndTaxes: 0.3064\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vec.id_to_term, topics=-1):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))\n",
    "    \n",
    "    for topic_idx, top_docs in model.top_topic_docs(doc_topic_matrix, topics=topic_idx, top_n=4, weights=True):\n",
    "        for doc, weight in top_docs:\n",
    "            print(\"\\t-{}: {:.4}\".format(t_list[doc]['Tag'], weight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
