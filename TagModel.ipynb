{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, make_scorer, accuracy_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/qaData.csv', parse_dates=['Date'])\n",
    "data['EarningTag2'] = data['EarningTag2'].str.strip()\n",
    "\n",
    "#Add Lagged Column\n",
    "data['Lag1'] = data.groupby([\"Company\", \"Participants\", \"Date\", \"EventName\", \"EventType\"])['EarningTag2'].shift(1)\n",
    "\n",
    "#Add Year and Month from Data\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "\n",
    "#Drop non-earning calls\n",
    "nn_data = data.loc[data['EventType']==\"Earnings call\", ['Company', 'Participants', 'Month', 'Year', 'AnalystName',\t'AnalystCompany', 'EventName', 'Lag1', 'EarningTag2']].copy()\n",
    "#Add quarter\n",
    "nn_data['Quarter'] = nn_data['EventName'].str.split(\"Q\").str[0]\n",
    "#Drop bad features\n",
    "nn_data = nn_data[['Company', \"Participants\", \"AnalystName\", \"AnalystCompany\", \"Month\", \"Year\", \"Quarter\", \"Lag1\", \"EarningTag2\"]].copy()\n",
    "\n",
    "#One-hot-encode categorical columns\n",
    "nn_data_encoded = pd.concat([nn_data, \n",
    "                             pd.get_dummies(nn_data['Company'], prefix='C', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(nn_data['Participants'], prefix='P', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(nn_data['AnalystName'], prefix='AN', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(nn_data['AnalystCompany'], prefix='AC', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(nn_data['Month'], prefix='M', prefix_sep=\"_\"),\n",
    "                             pd.get_dummies(nn_data['Quarter'], prefix='Q', prefix_sep=\"_\")], axis=1)\n",
    "\n",
    "new_cols = pd.get_dummies(nn_data['Company'], prefix='C', prefix_sep=\"_\").columns.tolist() + \\\n",
    "             pd.get_dummies(nn_data['Participants'], prefix='P', prefix_sep=\"_\").columns.tolist() + \\\n",
    "             pd.get_dummies(nn_data['AnalystName'], prefix='AN', prefix_sep=\"_\").columns.tolist() + \\\n",
    "             pd.get_dummies(nn_data['AnalystCompany'], prefix='AC', prefix_sep=\"_\").columns.tolist() + \\\n",
    "             pd.get_dummies(nn_data['Month'], prefix='M', prefix_sep=\"_\").columns.tolist() + \\\n",
    "             pd.get_dummies(nn_data['Quarter'], prefix='Q', prefix_sep=\"_\").columns.tolist()\n",
    "\n",
    "nn_data_encoded = nn_data_encoded[[\"Year\", \"Lag1\", \"EarningTag2\"] + new_cols].copy()\n",
    "new_cols = [col.replace(\" \", \"\") for col in new_cols]\n",
    "nn_data_encoded.columns = [\"Year\", \"Lag1\", \"EarningTag2\"] + new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data_encoded.dropna(inplace=True)\n",
    "nn_data_encoded.reset_index(inplace=True, drop=True)\n",
    "\n",
    "binarizer = LabelBinarizer().fit(nn_data_encoded['Lag1'])\n",
    "lag = pd.DataFrame(binarizer.transform(nn_data_encoded['Lag1']), \n",
    "                   columns=[\"lag_{}\".format(c) for c in binarizer.classes_])\n",
    "y = pd.DataFrame(binarizer.transform(nn_data_encoded['EarningTag2']), \n",
    "                   columns=[\"y_{}\".format(c) for c in binarizer.classes_])\n",
    "nn_data_encoded = pd.concat([nn_data_encoded, lag, y], axis=1)\n",
    "\n",
    "train = nn_data_encoded.loc[nn_data_encoded['Year']!=2018]\n",
    "test = nn_data_encoded.loc[nn_data_encoded['Year']==2018]\n",
    "\n",
    "X_train = train.drop([\"Year\", \"Lag1\", \"EarningTag2\"]+y.columns.tolist(), axis=1)\n",
    "X_test = test.drop([\"Year\", \"Lag1\", \"EarningTag2\"]+y.columns.tolist(), axis=1)\n",
    "y_train = train[y.columns].values\n",
    "y_test = test[y.columns].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = np.zeros(50)\n",
    "\n",
    "for pos, comp in enumerate(range(1, 50)):\n",
    "    model = NMF(n_components=comp)\n",
    "    X_train_W = model.fit_transform(X_train)\n",
    "    X_test_W = model.transform(X_test)\n",
    "\n",
    "    classifier = OneVsRestClassifier(RandomForestClassifier())\n",
    "    y_score = classifier.fit(X_train_W, y_train).predict_proba(X_test_W)\n",
    "\n",
    "    n_classes = y_score.shape[1]\n",
    "\n",
    "    lw=2\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    comps[pos] = roc_auc['macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6255890897219024"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=43)\n",
    "X_train_W = model.fit_transform(X_train)\n",
    "X_test_W = model.transform(X_test)\n",
    "classifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "y_score = classifier.fit(X_train_W, y_train).predict_proba(X_test_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data['NewIndex'] = nn_data['Company'].str.replace(\" \", \"\") +  \"_Y\" + nn_data['Year'].astype(str) + \"_M\" + nn_data['Month'].astype(str) + \"_Q\" + nn_data['Quarter'].astype(str)\n",
    "pct_data = (nn_data.groupby(['NewIndex', \"EarningTag2\"]).size().reset_index()).pivot(index='NewIndex', columns='EarningTag2', values=0).fillna(0)\n",
    "\n",
    "#pct_div_data = pct_data.div(pct_data.sum(axis=1), axis=0)\n",
    "pct_div_data = pct_data\n",
    "pct_div_data = pd.concat([pct_div_data.reset_index(drop=True), pct_div_data.reset_index()['NewIndex'].str.split(\"_\", expand=True)[[0, 1, 2, 3]]], axis=1, ignore_index=True)\n",
    "pct_div_data.columns = pct_data.columns.tolist() + ['Company', 'Year', 'Month', 'Quarter']\n",
    "pct_div_data = pct_div_data[['Company', 'Year', 'Month', 'Quarter'] + pct_data.columns.tolist()]\n",
    "\n",
    "pct_melt_data = pd.melt(pct_div_data, id_vars=['Company', 'Year', 'Month', 'Quarter'], var_name='Tag', value_name='NumQ')\n",
    "pct_melt_data = pd.concat([pct_melt_data, \n",
    "                             pd.get_dummies(pct_melt_data['Company'], prefix='C', prefix_sep=\"\"),\n",
    "                             pd.get_dummies(pct_melt_data['Month']),\n",
    "                             pd.get_dummies(pct_melt_data['Quarter']),\n",
    "                             pd.get_dummies(pct_melt_data['Year']),\n",
    "                             pd.get_dummies(pct_melt_data['Tag'], prefix='T', prefix_sep=\"\")], axis=1)\n",
    "pct_melt_data = pct_melt_data.drop(['Company', 'Year', 'Month', 'Quarter', 'Tag'], axis=1)\n",
    "pct_melt_data = pct_melt_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = pct_melt_data.loc[pct_melt_data['Y2018']!=1].copy().reset_index(drop=True), \\\n",
    "                pct_melt_data.loc[pct_melt_data['Y2018']==1].copy().reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = train.drop(['NumQ'], axis=1), train['NumQ'].values\n",
    "X_test, y_test = test.drop(['NumQ'], axis=1), test['NumQ'].values\n",
    "\n",
    "scores_gbc = np.zeros(50)\n",
    "scores_rf = np.zeros(50)\n",
    "\n",
    "for comp in range(1, 50):\n",
    "    model = NMF(n_components=comp)\n",
    "    X_train_W = model.fit_transform(X_train)\n",
    "    X_test_W = model.transform(X_test)\n",
    "    \n",
    "    estimator_gbc = GradientBoostingRegressor(warm_start=True).fit(X_train_W, y_train)\n",
    "    scores_gbc[comp] = mean_squared_error(y_test, estimator_gbc.predict(X_test_W).round())\n",
    "    \n",
    "    estimator_rf = RandomForestRegressor(warm_start=True).fit(X_train_W, y_train)\n",
    "    scores_rf[comp] = mean_squared_error(y_test, estimator_rf.predict(X_test_W).round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n"
     ]
    }
   ],
   "source": [
    "print('l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.011428571428572\n",
      "3.9085714285714284\n",
      "4.1571428571428575\n"
     ]
    }
   ],
   "source": [
    "print(scores[1:].min())\n",
    "print(scores_gbc[1:].min())\n",
    "print(scores_rf[1:].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2857142857142856"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_test = X_test.loc[(X_test['CBankofAmerica']==1)&(X_test['M1']==1)&(X_test['Q4']==1)].copy()\n",
    "y_test_test = y_test[X_test_test.index]\n",
    "\n",
    "\n",
    "model = NMF(n_components=26)\n",
    "X_train_W = model.fit_transform(X_train)\n",
    "X_test_test_W = model.transform(X_test_test)\n",
    "\n",
    "estimator_rf = GradientBoostingRegressor().fit(X_train_W, y_train)\n",
    "preds = estimator_rf.predict(X_test_test_W)\n",
    "mean_squared_error(y_test_test, preds.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 4., 1., 2., 2., 2., 2., 2., 1., 1., 2., 1., 1.])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27314814814814814"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All columns to string\n",
    "nn_data_str = nn_data.copy()\n",
    "nn_data_str['Year'] = nn_data_str['Year'].apply(str)\n",
    "nn_data_str['Quarter'] = nn_data_str['Quarter'].apply(str)\n",
    "nn_data_str['Month'] = nn_data_str['Month'].apply(str)\n",
    "\n",
    "#Remove spaces\n",
    "nn_data_str = nn_data_str.apply(lambda x: x.str.replace(\" \", \"\"), axis=1)\n",
    "\n",
    "#Train-Test Split\n",
    "train = nn_data_str.loc[nn_data_str['Year']!=\"2018\"]\n",
    "test = nn_data_str.loc[nn_data_str['Year']==\"2018\"]\n",
    "\n",
    "#X-y split\n",
    "X_train = train.drop('EarningTag2', axis=1).values\n",
    "X_train_str = [' '.join(x) for x in X_train]\n",
    "y_train = train['EarningTag2'].values\n",
    "\n",
    "X_test = test.drop(\"EarningTag2\", axis=1).values\n",
    "X_test_str = [' '.join(x) for x in X_test]\n",
    "y_test = test['EarningTag2'].values\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(lowercase=False).fit(X_train_str)\n",
    "X_train_tfidf = tfidf_vec.transform(X_train_str)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test_str)\n",
    "\n",
    "#Encode test\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "y_train_str = encoder.transform(y_train)\n",
    "y_test_str = encoder.transform(y_test)\n",
    "\n",
    "model = MultinomialNB().fit(X_train_tfidf, y_train_str)  \n",
    "preds = model.predict_proba(X_test_tfidf)\n",
    "accuracy_score(y_test_str, np.argmax(preds, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
